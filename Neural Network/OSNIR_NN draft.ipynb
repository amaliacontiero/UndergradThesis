{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import OSNIR Dataset and shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OSNIR dataset\n",
    "df = pd.read_csv('../Data/OSNIR_values_extendedv3_new datasetbcsv.csv')\n",
    "# shuffling rows of OSNIR dataframe and reset indexes\n",
    "df = df.sample(frac=1)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview of first 5 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OSNIRnumerical(dB)</th>\n",
       "      <th>Ns</th>\n",
       "      <th>Pch(dBm)</th>\n",
       "      <th>L(km)</th>\n",
       "      <th>B(GHz)</th>\n",
       "      <th>GB(GHz)</th>\n",
       "      <th>Nch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.655224</td>\n",
       "      <td>38</td>\n",
       "      <td>-3</td>\n",
       "      <td>100</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.920383</td>\n",
       "      <td>50</td>\n",
       "      <td>-7</td>\n",
       "      <td>50</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.138504</td>\n",
       "      <td>30</td>\n",
       "      <td>-2</td>\n",
       "      <td>50</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.459131</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.856131</td>\n",
       "      <td>19</td>\n",
       "      <td>-8</td>\n",
       "      <td>10</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OSNIRnumerical(dB)  Ns  Pch(dBm)  L(km)  B(GHz)  GB(GHz)  Nch\n",
       "0           10.655224  38        -3    100    12.5      0.0    9\n",
       "1           14.920383  50        -7     50    25.0      0.0    3\n",
       "2           18.138504  30        -2     50    25.0      0.0    3\n",
       "3            9.459131  36         1    100    25.0      0.0    9\n",
       "4           25.856131  19        -8     10    25.0      0.0    9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1800 values for training (0,1800-1)\\\n",
    "600 values for validation (1800,2400-1)\\\n",
    "600 values for testing (2400,3000-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input x aka Ns, Pch, L, B, GB, Nch values\n",
    "x_train = data[0:1800-1, 1:7]\n",
    "x_valid = data[1800:2400-1, 1:7]\n",
    "x_test = data[2400:3000-1, 1:7]\n",
    "\n",
    "# output y aka OSNIR values\n",
    "y_train = data[0:1800-1, 0]\n",
    "y_valid = data[1800:2400-1, 0]\n",
    "y_test = data[2400:3000-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 38. ,  -3. , 100. ,  12.5,   0. ,   9. ],\n",
       "       [ 50. ,  -7. ,  50. ,  25. ,   0. ,   3. ],\n",
       "       [ 30. ,  -2. ,  50. ,  25. ,   0. ,   3. ],\n",
       "       ...,\n",
       "       [ 48. ,   0. , 100. ,  12.5,   0. ,   9. ],\n",
       "       [ 41. ,   3. ,  50. ,  25. ,  25. ,   9. ],\n",
       "       [ 22. ,  -3. ,  50. ,  12.5,   0. ,   9. ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Neural Network\n",
    "1 Input layer, 2 hidden layers (32 neurons each) and 1 Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=(6,), activation='relu'),  # first hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # second hidden layer\n",
    "    keras.layers.Dense(1, activation='relu') # output layer (3)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early hault of training if loss has not improved in 50 epochs in validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Optimizer, Mean Square Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse'\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model, run for 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "72/72 [==============================] - 1s 3ms/step - loss: 105.8782 - val_loss: 54.2099\n",
      "Epoch 2/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 46.0105 - val_loss: 38.0755\n",
      "Epoch 3/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 35.1541 - val_loss: 32.8401\n",
      "Epoch 4/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 29.8212 - val_loss: 29.3665\n",
      "Epoch 5/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 27.1150 - val_loss: 28.1113\n",
      "Epoch 6/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 25.2967 - val_loss: 26.8154\n",
      "Epoch 7/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 23.1451 - val_loss: 25.6526\n",
      "Epoch 8/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 21.4167 - val_loss: 23.4821\n",
      "Epoch 9/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 20.1709 - val_loss: 23.1239\n",
      "Epoch 10/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 18.5513 - val_loss: 20.4955\n",
      "Epoch 11/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 17.3067 - val_loss: 20.8362\n",
      "Epoch 12/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 16.4618 - val_loss: 17.8848\n",
      "Epoch 13/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 15.6467 - val_loss: 16.8935\n",
      "Epoch 14/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 14.4134 - val_loss: 15.9829\n",
      "Epoch 15/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 13.6066 - val_loss: 15.0908\n",
      "Epoch 16/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 12.4524 - val_loss: 14.2760\n",
      "Epoch 17/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 11.7207 - val_loss: 13.7342\n",
      "Epoch 18/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 10.9614 - val_loss: 12.8323\n",
      "Epoch 19/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 10.4605 - val_loss: 11.5776\n",
      "Epoch 20/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 9.6505 - val_loss: 11.6270\n",
      "Epoch 21/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 9.1173 - val_loss: 10.4708\n",
      "Epoch 22/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8.8350 - val_loss: 10.5682\n",
      "Epoch 23/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7.8372 - val_loss: 9.0347\n",
      "Epoch 24/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7.4277 - val_loss: 8.2300\n",
      "Epoch 25/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.8693 - val_loss: 8.3755\n",
      "Epoch 26/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.2595 - val_loss: 7.8951\n",
      "Epoch 27/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.7349 - val_loss: 7.1523\n",
      "Epoch 28/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.5775 - val_loss: 6.6887\n",
      "Epoch 29/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.1312 - val_loss: 5.4778\n",
      "Epoch 30/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.6886 - val_loss: 5.0568\n",
      "Epoch 31/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.3618 - val_loss: 4.7054\n",
      "Epoch 32/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.1153 - val_loss: 4.5859\n",
      "Epoch 33/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.6413 - val_loss: 4.1310\n",
      "Epoch 34/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.5174 - val_loss: 3.6683\n",
      "Epoch 35/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.2328 - val_loss: 4.1308\n",
      "Epoch 36/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.0959 - val_loss: 3.2721\n",
      "Epoch 37/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.7947 - val_loss: 3.3572\n",
      "Epoch 38/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.6101 - val_loss: 3.3265\n",
      "Epoch 39/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.4180 - val_loss: 3.5154\n",
      "Epoch 40/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.3775 - val_loss: 3.2713\n",
      "Epoch 41/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.2918 - val_loss: 2.5820\n",
      "Epoch 42/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.1571 - val_loss: 2.5855\n",
      "Epoch 43/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.0670 - val_loss: 2.2384\n",
      "Epoch 44/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.8586 - val_loss: 2.3771\n",
      "Epoch 45/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.0078 - val_loss: 2.0352\n",
      "Epoch 46/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.7688 - val_loss: 2.1707\n",
      "Epoch 47/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.6047 - val_loss: 1.7820\n",
      "Epoch 48/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5906 - val_loss: 1.8338\n",
      "Epoch 49/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.4168 - val_loss: 2.3796\n",
      "Epoch 50/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5621 - val_loss: 1.7741\n",
      "Epoch 51/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5076 - val_loss: 1.4709\n",
      "Epoch 52/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.3315 - val_loss: 1.5357\n",
      "Epoch 53/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.2323 - val_loss: 1.3746\n",
      "Epoch 54/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2444 - val_loss: 1.3722\n",
      "Epoch 55/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2161 - val_loss: 1.3650\n",
      "Epoch 56/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1930 - val_loss: 1.3947\n",
      "Epoch 57/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1736 - val_loss: 1.2424\n",
      "Epoch 58/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2783 - val_loss: 1.1760\n",
      "Epoch 59/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.1502 - val_loss: 1.2192\n",
      "Epoch 60/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.0686 - val_loss: 1.0709\n",
      "Epoch 61/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.3129 - val_loss: 1.1581\n",
      "Epoch 62/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.0252 - val_loss: 1.2838\n",
      "Epoch 63/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.0379 - val_loss: 0.9824\n",
      "Epoch 64/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9224 - val_loss: 1.1661\n",
      "Epoch 65/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9767\n",
      "Epoch 66/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9504 - val_loss: 0.9971\n",
      "Epoch 67/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8902 - val_loss: 0.9685\n",
      "Epoch 68/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8533 - val_loss: 0.8137\n",
      "Epoch 69/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8449 - val_loss: 0.7909\n",
      "Epoch 70/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8753 - val_loss: 0.9472\n",
      "Epoch 71/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.9298 - val_loss: 0.8594\n",
      "Epoch 72/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7989 - val_loss: 0.7021\n",
      "Epoch 73/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7033 - val_loss: 0.7053\n",
      "Epoch 74/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6520 - val_loss: 0.6290\n",
      "Epoch 75/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7038 - val_loss: 1.0705\n",
      "Epoch 76/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6773 - val_loss: 0.6772\n",
      "Epoch 77/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6638 - val_loss: 0.5792\n",
      "Epoch 78/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7064 - val_loss: 0.6203\n",
      "Epoch 79/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6187 - val_loss: 0.6828\n",
      "Epoch 80/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6788 - val_loss: 0.5980\n",
      "Epoch 81/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6744 - val_loss: 0.6961\n",
      "Epoch 82/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6101 - val_loss: 0.5578\n",
      "Epoch 83/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7103 - val_loss: 0.8459\n",
      "Epoch 84/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6181 - val_loss: 0.5045\n",
      "Epoch 85/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5559 - val_loss: 0.6661\n",
      "Epoch 86/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.7698\n",
      "Epoch 87/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6301 - val_loss: 0.4513\n",
      "Epoch 88/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4549 - val_loss: 0.4477\n",
      "Epoch 89/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5204 - val_loss: 0.4139\n",
      "Epoch 90/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4866 - val_loss: 0.4286\n",
      "Epoch 91/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4534 - val_loss: 0.4257\n",
      "Epoch 92/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4589 - val_loss: 0.4598\n",
      "Epoch 93/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4429 - val_loss: 0.5905\n",
      "Epoch 94/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4693 - val_loss: 0.4701\n",
      "Epoch 95/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4762 - val_loss: 0.4194\n",
      "Epoch 96/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4468 - val_loss: 0.4982\n",
      "Epoch 97/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3842 - val_loss: 0.4478\n",
      "Epoch 98/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.3637 - val_loss: 0.4631\n",
      "Epoch 99/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6054 - val_loss: 0.3288\n",
      "Epoch 100/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4019 - val_loss: 0.5093\n",
      "Epoch 101/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4814 - val_loss: 0.4446\n",
      "Epoch 102/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4386 - val_loss: 0.3497\n",
      "Epoch 103/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.3842 - val_loss: 0.3560\n",
      "Epoch 104/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.4144 - val_loss: 0.4787\n",
      "Epoch 105/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4307 - val_loss: 0.3290\n",
      "Epoch 106/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.3622 - val_loss: 0.3220\n",
      "Epoch 107/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4044 - val_loss: 0.3846\n",
      "Epoch 108/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3982 - val_loss: 0.4786\n",
      "Epoch 109/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3360 - val_loss: 0.4175\n",
      "Epoch 110/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3828 - val_loss: 0.3392\n",
      "Epoch 111/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3533 - val_loss: 0.2936\n",
      "Epoch 112/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3565 - val_loss: 0.3897\n",
      "Epoch 113/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.2859\n",
      "Epoch 114/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3164 - val_loss: 0.2830\n",
      "Epoch 115/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.5967\n",
      "Epoch 116/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3781 - val_loss: 0.2806\n",
      "Epoch 117/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3635 - val_loss: 0.4349\n",
      "Epoch 118/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3795 - val_loss: 0.3433\n",
      "Epoch 119/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3492 - val_loss: 0.2904\n",
      "Epoch 120/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3434 - val_loss: 0.2832\n",
      "Epoch 121/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3204 - val_loss: 0.2980\n",
      "Epoch 122/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3119 - val_loss: 0.3373\n",
      "Epoch 123/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3113 - val_loss: 0.3008\n",
      "Epoch 124/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3245 - val_loss: 0.2387\n",
      "Epoch 125/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3512 - val_loss: 0.4497\n",
      "Epoch 126/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3882 - val_loss: 0.3377\n",
      "Epoch 127/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3265 - val_loss: 0.2815\n",
      "Epoch 128/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3111 - val_loss: 0.3277\n",
      "Epoch 129/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3160 - val_loss: 0.2728\n",
      "Epoch 130/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3068 - val_loss: 0.2530\n",
      "Epoch 131/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3522 - val_loss: 0.2531\n",
      "Epoch 132/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2851 - val_loss: 0.2987\n",
      "Epoch 133/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3075 - val_loss: 0.3398\n",
      "Epoch 134/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3271 - val_loss: 0.2653\n",
      "Epoch 135/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3343 - val_loss: 0.2598\n",
      "Epoch 136/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2936 - val_loss: 0.3107\n",
      "Epoch 137/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3443 - val_loss: 0.2514\n",
      "Epoch 138/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2956 - val_loss: 0.2452\n",
      "Epoch 139/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2666 - val_loss: 0.2422\n",
      "Epoch 140/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3450 - val_loss: 0.2851\n",
      "Epoch 141/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2979 - val_loss: 0.2653\n",
      "Epoch 142/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3259 - val_loss: 0.2646\n",
      "Epoch 143/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2734 - val_loss: 0.2378\n",
      "Epoch 144/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3066 - val_loss: 0.2801\n",
      "Epoch 145/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3343 - val_loss: 0.2298\n",
      "Epoch 146/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3260 - val_loss: 0.2507\n",
      "Epoch 147/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2968 - val_loss: 0.4925\n",
      "Epoch 148/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2885 - val_loss: 0.2242\n",
      "Epoch 149/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2558 - val_loss: 0.4753\n",
      "Epoch 150/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2799 - val_loss: 0.2569\n",
      "Epoch 151/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2885 - val_loss: 0.2732\n",
      "Epoch 152/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2422 - val_loss: 0.2373\n",
      "Epoch 153/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2657 - val_loss: 0.3171\n",
      "Epoch 154/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2648 - val_loss: 0.2970\n",
      "Epoch 155/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2524 - val_loss: 0.2072\n",
      "Epoch 156/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2502 - val_loss: 0.2629\n",
      "Epoch 157/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3407 - val_loss: 0.5115\n",
      "Epoch 158/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2728 - val_loss: 0.2917\n",
      "Epoch 159/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2683 - val_loss: 0.4290\n",
      "Epoch 160/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2622 - val_loss: 0.2538\n",
      "Epoch 161/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2575 - val_loss: 0.2414\n",
      "Epoch 162/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2602 - val_loss: 0.3576\n",
      "Epoch 163/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2684 - val_loss: 0.3058\n",
      "Epoch 164/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2505 - val_loss: 0.3351\n",
      "Epoch 165/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2970 - val_loss: 0.2336\n",
      "Epoch 166/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2420 - val_loss: 0.2109\n",
      "Epoch 167/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2243 - val_loss: 0.4213\n",
      "Epoch 168/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2952 - val_loss: 0.3224\n",
      "Epoch 169/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2597 - val_loss: 0.2626\n",
      "Epoch 170/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2972 - val_loss: 0.2379\n",
      "Epoch 171/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3166 - val_loss: 0.3157\n",
      "Epoch 172/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2554 - val_loss: 0.2495\n",
      "Epoch 173/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2403 - val_loss: 0.2634\n",
      "Epoch 174/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2543 - val_loss: 0.3927\n",
      "Epoch 175/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2443 - val_loss: 0.2022\n",
      "Epoch 176/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2356 - val_loss: 0.3203\n",
      "Epoch 177/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3122 - val_loss: 0.4528\n",
      "Epoch 178/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.2828\n",
      "Epoch 179/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2271 - val_loss: 0.3776\n",
      "Epoch 180/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2465 - val_loss: 0.2168\n",
      "Epoch 181/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2103 - val_loss: 0.2169\n",
      "Epoch 182/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2305 - val_loss: 0.2138\n",
      "Epoch 183/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2129 - val_loss: 0.3553\n",
      "Epoch 184/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2041 - val_loss: 0.2120\n",
      "Epoch 185/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2725 - val_loss: 0.3930\n",
      "Epoch 186/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1988 - val_loss: 0.2878\n",
      "Epoch 187/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2268 - val_loss: 0.1887\n",
      "Epoch 188/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3078 - val_loss: 1.3007\n",
      "Epoch 189/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3295 - val_loss: 0.2849\n",
      "Epoch 190/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2730 - val_loss: 0.1869\n",
      "Epoch 191/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1846 - val_loss: 0.1723\n",
      "Epoch 192/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2003 - val_loss: 0.4095\n",
      "Epoch 193/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2853 - val_loss: 0.2731\n",
      "Epoch 194/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1803 - val_loss: 0.2579\n",
      "Epoch 195/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2742 - val_loss: 0.2331\n",
      "Epoch 196/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2045 - val_loss: 0.2336\n",
      "Epoch 197/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 0.2635\n",
      "Epoch 198/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2176 - val_loss: 0.1884\n",
      "Epoch 199/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1952 - val_loss: 0.3553\n",
      "Epoch 200/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2804 - val_loss: 0.1794\n",
      "Epoch 201/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1882 - val_loss: 0.2768\n",
      "Epoch 202/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2429 - val_loss: 0.2592\n",
      "Epoch 203/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2583 - val_loss: 0.1878\n",
      "Epoch 204/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2633 - val_loss: 0.2443\n",
      "Epoch 205/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2020 - val_loss: 0.2236\n",
      "Epoch 206/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1956 - val_loss: 0.1988\n",
      "Epoch 207/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2055 - val_loss: 0.1904\n",
      "Epoch 208/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2644 - val_loss: 0.5057\n",
      "Epoch 209/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2768 - val_loss: 0.2114\n",
      "Epoch 210/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 0.2060\n",
      "Epoch 211/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 0.3202\n",
      "Epoch 212/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1919 - val_loss: 0.1661\n",
      "Epoch 213/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1647 - val_loss: 0.1949\n",
      "Epoch 214/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2008 - val_loss: 0.2352\n",
      "Epoch 215/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2108 - val_loss: 0.2162\n",
      "Epoch 216/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1861 - val_loss: 0.2626\n",
      "Epoch 217/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2026 - val_loss: 0.1592\n",
      "Epoch 218/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1791 - val_loss: 0.1872\n",
      "Epoch 219/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2076 - val_loss: 0.1544\n",
      "Epoch 220/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2350 - val_loss: 0.2503\n",
      "Epoch 221/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1773 - val_loss: 0.2266\n",
      "Epoch 222/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2289 - val_loss: 0.1646\n",
      "Epoch 223/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1595 - val_loss: 0.1934\n",
      "Epoch 224/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2209 - val_loss: 0.2028\n",
      "Epoch 225/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1786 - val_loss: 0.2028\n",
      "Epoch 226/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2295 - val_loss: 0.2961\n",
      "Epoch 227/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2074 - val_loss: 0.1591\n",
      "Epoch 228/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2061 - val_loss: 0.1869\n",
      "Epoch 229/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2044 - val_loss: 0.2408\n",
      "Epoch 230/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1936 - val_loss: 0.1907\n",
      "Epoch 231/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2501 - val_loss: 0.3209\n",
      "Epoch 232/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3134 - val_loss: 0.2448\n",
      "Epoch 233/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1598 - val_loss: 0.1404\n",
      "Epoch 234/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2607 - val_loss: 0.4235\n",
      "Epoch 235/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2009 - val_loss: 0.4106\n",
      "Epoch 236/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1918 - val_loss: 0.2280\n",
      "Epoch 237/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2023 - val_loss: 0.2051\n",
      "Epoch 238/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1854 - val_loss: 0.1707\n",
      "Epoch 239/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1759 - val_loss: 0.1939\n",
      "Epoch 240/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.1361\n",
      "Epoch 241/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1614 - val_loss: 0.1690\n",
      "Epoch 242/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1875 - val_loss: 0.1712\n",
      "Epoch 243/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2402 - val_loss: 0.1625\n",
      "Epoch 244/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1559 - val_loss: 0.1700\n",
      "Epoch 245/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2263 - val_loss: 0.4615\n",
      "Epoch 246/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 0.2612\n",
      "Epoch 247/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1493 - val_loss: 0.2134\n",
      "Epoch 248/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2080 - val_loss: 0.1689\n",
      "Epoch 249/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1496 - val_loss: 0.1731\n",
      "Epoch 250/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.1363\n",
      "Epoch 251/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1539 - val_loss: 0.1608\n",
      "Epoch 252/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2161 - val_loss: 0.3878\n",
      "Epoch 253/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2128 - val_loss: 0.2661\n",
      "Epoch 254/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2012 - val_loss: 0.2524\n",
      "Epoch 255/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1991 - val_loss: 0.1669\n",
      "Epoch 256/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 0.1514\n",
      "Epoch 257/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1762 - val_loss: 0.2921\n",
      "Epoch 258/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1717 - val_loss: 0.1473\n",
      "Epoch 259/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1785 - val_loss: 0.1700\n",
      "Epoch 260/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1852 - val_loss: 0.1484\n",
      "Epoch 261/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1420 - val_loss: 0.1480\n",
      "Epoch 262/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1841 - val_loss: 0.1262\n",
      "Epoch 263/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1523 - val_loss: 0.1472\n",
      "Epoch 264/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1650 - val_loss: 0.1912\n",
      "Epoch 265/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1673 - val_loss: 0.1647\n",
      "Epoch 266/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2047 - val_loss: 0.1576\n",
      "Epoch 267/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2516 - val_loss: 0.2204\n",
      "Epoch 268/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1721 - val_loss: 0.1696\n",
      "Epoch 269/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1906 - val_loss: 0.1557\n",
      "Epoch 270/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1975 - val_loss: 0.2371\n",
      "Epoch 271/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1686 - val_loss: 0.1443\n",
      "Epoch 272/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1822 - val_loss: 0.1747\n",
      "Epoch 273/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1426 - val_loss: 0.1236\n",
      "Epoch 274/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 0.1738\n",
      "Epoch 275/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1859 - val_loss: 0.2012\n",
      "Epoch 276/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1348 - val_loss: 0.3130\n",
      "Epoch 277/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1855 - val_loss: 0.2089\n",
      "Epoch 278/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 0.2425\n",
      "Epoch 279/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1484 - val_loss: 0.1799\n",
      "Epoch 280/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2386 - val_loss: 0.1643\n",
      "Epoch 281/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1369 - val_loss: 0.1358\n",
      "Epoch 282/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1363 - val_loss: 0.1161\n",
      "Epoch 283/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1502\n",
      "Epoch 284/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1636 - val_loss: 0.4459\n",
      "Epoch 285/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1684 - val_loss: 0.5005\n",
      "Epoch 286/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1673 - val_loss: 0.1517\n",
      "Epoch 287/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1438 - val_loss: 0.1366\n",
      "Epoch 288/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1842 - val_loss: 0.1797\n",
      "Epoch 289/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1687 - val_loss: 0.1257\n",
      "Epoch 290/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1489 - val_loss: 0.1123\n",
      "Epoch 291/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.1446\n",
      "Epoch 292/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1514 - val_loss: 0.1322\n",
      "Epoch 293/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.1507\n",
      "Epoch 294/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1473 - val_loss: 0.2182\n",
      "Epoch 295/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1349 - val_loss: 0.1290\n",
      "Epoch 296/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1654 - val_loss: 0.1599\n",
      "Epoch 297/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1754 - val_loss: 0.1857\n",
      "Epoch 298/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1483 - val_loss: 0.1248\n",
      "Epoch 299/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1654 - val_loss: 0.1388\n",
      "Epoch 300/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1809 - val_loss: 0.1314\n",
      "Epoch 301/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1591 - val_loss: 0.1565\n",
      "Epoch 302/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1512 - val_loss: 0.3290\n",
      "Epoch 303/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1476 - val_loss: 0.1324\n",
      "Epoch 304/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2269 - val_loss: 0.1657\n",
      "Epoch 305/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 0.1425\n",
      "Epoch 306/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1307 - val_loss: 0.1475\n",
      "Epoch 307/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1600 - val_loss: 0.1657\n",
      "Epoch 308/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 0.1499\n",
      "Epoch 309/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1395 - val_loss: 0.1610\n",
      "Epoch 310/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.1454\n",
      "Epoch 311/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1867 - val_loss: 0.3404\n",
      "Epoch 312/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 0.1584\n",
      "Epoch 313/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1472 - val_loss: 0.1358\n",
      "Epoch 314/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1509 - val_loss: 0.1801\n",
      "Epoch 315/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1290 - val_loss: 0.1352\n",
      "Epoch 316/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1763 - val_loss: 0.1269\n",
      "Epoch 317/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1458 - val_loss: 0.1664\n",
      "Epoch 318/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1968 - val_loss: 0.2010\n",
      "Epoch 319/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1424 - val_loss: 0.1442\n",
      "Epoch 320/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1335 - val_loss: 0.1868\n",
      "Epoch 321/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.1551\n",
      "Epoch 322/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 0.1570\n",
      "Epoch 323/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1436 - val_loss: 0.1640\n",
      "Epoch 324/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1464 - val_loss: 0.3290\n",
      "Epoch 325/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.2409\n",
      "Epoch 326/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1381 - val_loss: 0.1175\n",
      "Epoch 327/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.1094\n",
      "Epoch 328/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1648 - val_loss: 0.3958\n",
      "Epoch 329/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 0.1369\n",
      "Epoch 330/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1369 - val_loss: 0.2244\n",
      "Epoch 331/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2007 - val_loss: 0.1813\n",
      "Epoch 332/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1610 - val_loss: 0.1746\n",
      "Epoch 333/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1068 - val_loss: 0.1250\n",
      "Epoch 334/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1109 - val_loss: 0.1128\n",
      "Epoch 335/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1449 - val_loss: 0.1167\n",
      "Epoch 336/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1217 - val_loss: 0.1380\n",
      "Epoch 337/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1212 - val_loss: 0.1647\n",
      "Epoch 338/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.0922\n",
      "Epoch 339/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1117 - val_loss: 0.1672\n",
      "Epoch 340/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1355 - val_loss: 0.1411\n",
      "Epoch 341/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1715 - val_loss: 0.3861\n",
      "Epoch 342/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1789 - val_loss: 0.2123\n",
      "Epoch 343/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2036 - val_loss: 0.1364\n",
      "Epoch 344/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1377 - val_loss: 0.1859\n",
      "Epoch 345/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1693 - val_loss: 0.1039\n",
      "Epoch 346/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.0992\n",
      "Epoch 347/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 0.1149\n",
      "Epoch 348/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1123 - val_loss: 0.1268\n",
      "Epoch 349/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1303\n",
      "Epoch 350/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 0.1880\n",
      "Epoch 351/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1124 - val_loss: 0.1334\n",
      "Epoch 352/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1583 - val_loss: 0.1486\n",
      "Epoch 353/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.1940\n",
      "Epoch 354/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1500 - val_loss: 0.1190\n",
      "Epoch 355/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1332\n",
      "Epoch 356/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1414 - val_loss: 0.2059\n",
      "Epoch 357/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.1025\n",
      "Epoch 358/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0886 - val_loss: 0.1147\n",
      "Epoch 359/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1061 - val_loss: 0.1406\n",
      "Epoch 360/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.1407\n",
      "Epoch 361/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1158 - val_loss: 0.1293\n",
      "Epoch 362/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 0.1208\n",
      "Epoch 363/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1229 - val_loss: 0.1530\n",
      "Epoch 364/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 0.1429\n",
      "Epoch 365/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2038 - val_loss: 0.1487\n",
      "Epoch 366/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.1090\n",
      "Epoch 367/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1203 - val_loss: 0.1183\n",
      "Epoch 368/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1393 - val_loss: 0.1469\n",
      "Epoch 369/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.1240\n",
      "Epoch 370/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 0.1581\n",
      "Epoch 371/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.1285\n",
      "Epoch 372/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1193 - val_loss: 0.1263\n",
      "Epoch 373/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1587 - val_loss: 0.3822\n",
      "Epoch 374/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1151 - val_loss: 0.1264\n",
      "Epoch 375/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.1165\n",
      "Epoch 376/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1216 - val_loss: 0.2341\n",
      "Epoch 377/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1187 - val_loss: 0.1574\n",
      "Epoch 378/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.1025\n",
      "Epoch 379/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1102 - val_loss: 0.0914\n",
      "Epoch 380/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.1948\n",
      "Epoch 381/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1359 - val_loss: 0.1601\n",
      "Epoch 382/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1747 - val_loss: 0.1615\n",
      "Epoch 383/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1067 - val_loss: 0.0892\n",
      "Epoch 384/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0889 - val_loss: 0.1090\n",
      "Epoch 385/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1426 - val_loss: 0.2312\n",
      "Epoch 386/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1745 - val_loss: 0.2133\n",
      "Epoch 387/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.1043\n",
      "Epoch 388/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1226 - val_loss: 0.1858\n",
      "Epoch 389/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1020 - val_loss: 0.1121\n",
      "Epoch 390/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.2376\n",
      "Epoch 391/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1577 - val_loss: 0.2497\n",
      "Epoch 392/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1148 - val_loss: 0.1283\n",
      "Epoch 393/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1097 - val_loss: 0.1017\n",
      "Epoch 394/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 0.1070\n",
      "Epoch 395/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1083\n",
      "Epoch 396/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1365 - val_loss: 0.1706\n",
      "Epoch 397/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1181 - val_loss: 0.1730\n",
      "Epoch 398/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1235 - val_loss: 0.1235\n",
      "Epoch 399/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1566 - val_loss: 0.1005\n",
      "Epoch 400/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0913 - val_loss: 0.1273\n",
      "Epoch 401/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1178 - val_loss: 0.1190\n",
      "Epoch 402/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1010 - val_loss: 0.1754\n",
      "Epoch 403/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1926 - val_loss: 0.3927\n",
      "Epoch 404/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2126 - val_loss: 0.2181\n",
      "Epoch 405/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1725 - val_loss: 0.1412\n",
      "Epoch 406/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 0.1231\n",
      "Epoch 407/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.0996\n",
      "Epoch 408/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0957 - val_loss: 0.1248\n",
      "Epoch 409/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0959 - val_loss: 0.1633\n",
      "Epoch 410/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.1073\n",
      "Epoch 411/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 0.1101\n",
      "Epoch 412/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1139 - val_loss: 0.2080\n",
      "Epoch 413/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1130 - val_loss: 0.2075\n",
      "Epoch 414/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 0.1107\n",
      "Epoch 415/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.1016\n",
      "Epoch 416/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 0.2508\n",
      "Epoch 417/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1608 - val_loss: 0.1432\n",
      "Epoch 418/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1378 - val_loss: 0.1565\n",
      "Epoch 419/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1622 - val_loss: 0.1530\n",
      "Epoch 420/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1154\n",
      "Epoch 421/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1023 - val_loss: 0.0955\n",
      "Epoch 422/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.1432\n",
      "Epoch 423/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 0.1792\n",
      "Epoch 424/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1197 - val_loss: 0.1483\n",
      "Epoch 425/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1140 - val_loss: 0.0969\n",
      "Epoch 426/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 0.1206\n",
      "Epoch 427/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1143 - val_loss: 0.0895\n",
      "Epoch 428/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1008 - val_loss: 0.0876\n",
      "Epoch 429/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1457 - val_loss: 0.1337\n",
      "Epoch 430/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1331 - val_loss: 0.1338\n",
      "Epoch 431/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.0897\n",
      "Epoch 432/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0836 - val_loss: 0.0932\n",
      "Epoch 433/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1125 - val_loss: 0.1165\n",
      "Epoch 434/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1038 - val_loss: 0.2177\n",
      "Epoch 435/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.1077\n",
      "Epoch 436/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0942 - val_loss: 0.0970\n",
      "Epoch 437/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.1649\n",
      "Epoch 438/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0992 - val_loss: 0.1214\n",
      "Epoch 439/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1286 - val_loss: 0.1258\n",
      "Epoch 440/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 0.1289\n",
      "Epoch 441/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0971 - val_loss: 0.1486\n",
      "Epoch 442/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.0820\n",
      "Epoch 443/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0891 - val_loss: 0.0946\n",
      "Epoch 444/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1154 - val_loss: 0.2019\n",
      "Epoch 445/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.0770\n",
      "Epoch 446/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1138 - val_loss: 0.1211\n",
      "Epoch 447/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1168 - val_loss: 0.0866\n",
      "Epoch 448/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.1252\n",
      "Epoch 449/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1122 - val_loss: 0.1358\n",
      "Epoch 450/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0897 - val_loss: 0.1007\n",
      "Epoch 451/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.0926\n",
      "Epoch 452/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1259 - val_loss: 0.1217\n",
      "Epoch 453/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.1963\n",
      "Epoch 454/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.1196\n",
      "Epoch 455/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0977 - val_loss: 0.1099\n",
      "Epoch 456/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0910 - val_loss: 0.0828\n",
      "Epoch 457/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.1346\n",
      "Epoch 458/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.1535\n",
      "Epoch 459/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 0.1569\n",
      "Epoch 460/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.0869\n",
      "Epoch 461/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1031 - val_loss: 0.1094\n",
      "Epoch 462/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1055 - val_loss: 0.1046\n",
      "Epoch 463/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0843 - val_loss: 0.0956\n",
      "Epoch 464/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 0.2118\n",
      "Epoch 465/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1289 - val_loss: 0.0840\n",
      "Epoch 466/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0759 - val_loss: 0.1081\n",
      "Epoch 467/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.1117\n",
      "Epoch 468/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 0.1697\n",
      "Epoch 469/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1167 - val_loss: 0.1431\n",
      "Epoch 470/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1218 - val_loss: 0.1195\n",
      "Epoch 471/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1447 - val_loss: 0.1354\n",
      "Epoch 472/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0860 - val_loss: 0.1004\n",
      "Epoch 473/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 0.1167\n",
      "Epoch 474/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.0845\n",
      "Epoch 475/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.0834\n",
      "Epoch 476/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0933 - val_loss: 0.1567\n",
      "Epoch 477/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0965 - val_loss: 0.0889\n",
      "Epoch 478/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0936 - val_loss: 0.1096\n",
      "Epoch 479/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0929 - val_loss: 0.1056\n",
      "Epoch 480/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1249 - val_loss: 0.1240\n",
      "Epoch 481/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0860 - val_loss: 0.0910\n",
      "Epoch 482/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0761 - val_loss: 0.0768\n",
      "Epoch 483/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0900 - val_loss: 0.0923\n",
      "Epoch 484/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1444 - val_loss: 0.1063\n",
      "Epoch 485/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1318 - val_loss: 0.0985\n",
      "Epoch 486/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.1398\n",
      "Epoch 487/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1136 - val_loss: 0.1169\n",
      "Epoch 488/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.0939\n",
      "Epoch 489/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1785\n",
      "Epoch 490/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.1001\n",
      "Epoch 491/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0902 - val_loss: 0.1818\n",
      "Epoch 492/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.1482\n",
      "Epoch 493/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 0.1884\n",
      "Epoch 494/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1108 - val_loss: 0.0983\n",
      "Epoch 495/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 0.1076\n",
      "Epoch 496/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1274 - val_loss: 0.1632\n",
      "Epoch 497/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1158 - val_loss: 0.0917\n",
      "Epoch 498/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.0766\n",
      "Epoch 499/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0969 - val_loss: 0.1867\n",
      "Epoch 500/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.0815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2267aa00f90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_valid,y_valid), \n",
    "          epochs=500, \n",
    "          batch_size=25,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSNIR_num = data[:3000-1,0]\n",
    "Input_num = data[:3000-1,1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "OSNIR_est = (model.predict(Input_num)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.0, -3.0, 100.0, 12.5, 0.0, 9.0] => 11.174129 (expected 10.655224)\n",
      "[50.0, -7.0, 50.0, 25.0, 0.0, 3.0] => 15.434303 (expected 14.920383)\n",
      "[30.0, -2.0, 50.0, 25.0, 0.0, 3.0] => 18.075792 (expected 18.138504)\n",
      "[36.0, 1.0, 100.0, 25.0, 0.0, 9.0] => 9.255724 (expected 9.459131)\n",
      "[19.0, -8.0, 10.0, 25.0, 0.0, 9.0] => 25.379766 (expected 25.856131)\n",
      "[24.0, -4.0, 100.0, 12.5, 0.0, 9.0] => 12.648388 (expected 12.804099)\n",
      "[25.0, -7.0, 100.0, 25.0, 0.0, 9.0] => 7.791731 (expected 7.938533)\n",
      "[29.0, 3.0, 100.0, 25.0, 25.0, 9.0] => 9.921718 (expected 9.551508)\n",
      "[27.0, 3.0, 10.0, 50.0, 0.0, 9.0] => 17.149124 (expected 17.620399)\n",
      "[13.0, 3.0, 100.0, 25.0, 12.5, 9.0] => 13.806187 (expected 13.479461)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('%s => %f (expected %f)' % (Input_num[i].tolist(), OSNIR_est[i], OSNIR_num[i] ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate difference between actual OSNIR and predicted OSNIR (in dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mism = np.zeros(3000-1)\n",
    "for i in range(3000-1):\n",
    "    Mism[i] = OSNIR_num[i] - OSNIR_est[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Actual OSNIR (dB)\" : OSNIR_num.tolist(), \"Predicted OSNIR (dB)\" : OSNIR_est.tolist(), \"Difference (dB)\" : Mism.tolist()})\n",
    "pred_df.to_csv(\"OSNIR_Prediction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   2.,   2.,  16., 112., 283., 576.,\n",
       "        825., 681., 337., 122.,  27.,   9.,   4.,   0.,   0.,   2.,   0.,\n",
       "          0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.]),\n",
       " array([-4.        , -3.82222222, -3.64444444, -3.46666667, -3.28888889,\n",
       "        -3.11111111, -2.93333333, -2.75555556, -2.57777778, -2.4       ,\n",
       "        -2.22222222, -2.04444444, -1.86666667, -1.68888889, -1.51111111,\n",
       "        -1.33333333, -1.15555556, -0.97777778, -0.8       , -0.62222222,\n",
       "        -0.44444444, -0.26666667, -0.08888889,  0.08888889,  0.26666667,\n",
       "         0.44444444,  0.62222222,  0.8       ,  0.97777778,  1.15555556,\n",
       "         1.33333333,  1.51111111,  1.68888889,  1.86666667,  2.04444444,\n",
       "         2.22222222,  2.4       ,  2.57777778,  2.75555556,  2.93333333,\n",
       "         3.11111111,  3.28888889,  3.46666667,  3.64444444,  3.82222222,\n",
       "         4.        ]),\n",
       " <BarContainer object of 45 artists>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Deep Neural Network OSNIR Mismatch')\n",
    "plt.xlabel('Mismatch (dB)')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(800))\n",
    "plt.hist(Mism, range=(-4,4), rwidth=0.9, bins=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd7dbc09f62c6934dc245b76251cec5fd949cd1cb8bcb775af9208cac5a10da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
