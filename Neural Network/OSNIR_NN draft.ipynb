{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import OSNIR Dataset and shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OSNIR dataset\n",
    "df = pd.read_csv('../Data/OSNIR_values_extendedv3_new datasetbcsv.csv')\n",
    "# shuffling rows of OSNIR dataframe and reset indexes\n",
    "df = df.sample(frac=1)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview of first 5 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OSNIRnumerical(dB)</th>\n",
       "      <th>Ns</th>\n",
       "      <th>Pch(dBm)</th>\n",
       "      <th>L(km)</th>\n",
       "      <th>B(GHz)</th>\n",
       "      <th>GB(GHz)</th>\n",
       "      <th>Nch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.498140</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.681568</td>\n",
       "      <td>27</td>\n",
       "      <td>-9</td>\n",
       "      <td>100</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.521156</td>\n",
       "      <td>26</td>\n",
       "      <td>-4</td>\n",
       "      <td>100</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.015920</td>\n",
       "      <td>18</td>\n",
       "      <td>-4</td>\n",
       "      <td>50</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.361415</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>100</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OSNIRnumerical(dB)  Ns  Pch(dBm)  L(km)  B(GHz)  GB(GHz)  Nch\n",
       "0           18.498140  37         0     10    25.0      0.0    3\n",
       "1            5.681568  27        -9    100    25.0      0.0    9\n",
       "2           10.521156  26        -4    100    25.0      0.0    9\n",
       "3           21.015920  18        -4     50    25.0      0.0    3\n",
       "4           10.361415  37        -1    100    25.0      0.0    9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1800 values for training (0,1800-1)\\\n",
    "600 values for validation (1800,2400-1)\\\n",
    "600 values for testing (2400,3000-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input x aka Ns, Pch, L, B, GB, Nch values\n",
    "x_train = data[0:1800-1, 1:7]\n",
    "x_valid = data[1800:2400-1, 1:7]\n",
    "x_test = data[2400:3000-1, 1:7]\n",
    "\n",
    "# output y aka OSNIR values\n",
    "y_train = data[0:1800-1, 0]\n",
    "y_valid = data[1800:2400-1, 0]\n",
    "y_test = data[2400:3000-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.,   0.,  10.,  25.,   0.,   3.],\n",
       "       [ 27.,  -9., 100.,  25.,   0.,   9.],\n",
       "       [ 26.,  -4., 100.,  25.,   0.,   9.],\n",
       "       ...,\n",
       "       [ 14.,   3.,  50.,  25.,  25.,   9.],\n",
       "       [ 11.,  -1.,  10.,  25.,   0.,   9.],\n",
       "       [ 46.,   0.,  10.,  25.,   0.,  15.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Neural Network\n",
    "1 Input layer, 2 hidden layers (32 neurons each) and 1 Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=(6,), activation='relu'),  # first hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # second hidden layer\n",
    "    keras.layers.Dense(1, activation='relu') # output layer (3)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early hault of training if loss has not improved in 50 epochs in validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Optimizer, Mean Square Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse'\n",
    "              #metrics=['accuracy']\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model, run for 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "72/72 [==============================] - 3s 10ms/step - loss: 85.6883 - val_loss: 54.9849\n",
      "Epoch 2/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 51.2880 - val_loss: 45.0940\n",
      "Epoch 3/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 41.0127 - val_loss: 34.0249\n",
      "Epoch 4/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 34.0784 - val_loss: 29.4725\n",
      "Epoch 5/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 29.7843 - val_loss: 26.7524\n",
      "Epoch 6/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 27.3939 - val_loss: 24.8348\n",
      "Epoch 7/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 25.6637 - val_loss: 23.3715\n",
      "Epoch 8/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 24.2274 - val_loss: 22.2775\n",
      "Epoch 9/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 22.9117 - val_loss: 21.9414\n",
      "Epoch 10/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 21.5788 - val_loss: 19.8101\n",
      "Epoch 11/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 20.0675 - val_loss: 18.8093\n",
      "Epoch 12/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 18.8064 - val_loss: 18.6375\n",
      "Epoch 13/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 17.6717 - val_loss: 16.3839\n",
      "Epoch 14/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 16.5488 - val_loss: 16.5376\n",
      "Epoch 15/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 15.5653 - val_loss: 14.3063\n",
      "Epoch 16/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 14.3361 - val_loss: 13.2945\n",
      "Epoch 17/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 13.3098 - val_loss: 13.0642\n",
      "Epoch 18/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 12.1868 - val_loss: 11.5113\n",
      "Epoch 19/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 11.3233 - val_loss: 10.9047\n",
      "Epoch 20/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 10.3255 - val_loss: 9.4239\n",
      "Epoch 21/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 9.5855 - val_loss: 8.5236\n",
      "Epoch 22/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8.9424 - val_loss: 8.7774\n",
      "Epoch 23/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8.3710 - val_loss: 7.5635\n",
      "Epoch 24/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7.8336 - val_loss: 7.3959\n",
      "Epoch 25/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7.3540 - val_loss: 6.5793\n",
      "Epoch 26/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.8801 - val_loss: 6.4017\n",
      "Epoch 27/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.6475 - val_loss: 6.0389\n",
      "Epoch 28/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.3889 - val_loss: 5.6480\n",
      "Epoch 29/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.9247 - val_loss: 5.3736\n",
      "Epoch 30/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.3565 - val_loss: 4.8742\n",
      "Epoch 31/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.5482 - val_loss: 4.9037\n",
      "Epoch 32/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.9906 - val_loss: 4.2511\n",
      "Epoch 33/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.7986 - val_loss: 4.3030\n",
      "Epoch 34/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.5068 - val_loss: 3.8985\n",
      "Epoch 35/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.3514 - val_loss: 3.5136\n",
      "Epoch 36/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.9098 - val_loss: 3.6355\n",
      "Epoch 37/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.8347 - val_loss: 3.3588\n",
      "Epoch 38/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.6638 - val_loss: 3.0130\n",
      "Epoch 39/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.5744 - val_loss: 2.9925\n",
      "Epoch 40/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.3121 - val_loss: 3.5319\n",
      "Epoch 41/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.1692 - val_loss: 2.5362\n",
      "Epoch 42/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.8882 - val_loss: 2.2980\n",
      "Epoch 43/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.7839 - val_loss: 2.2443\n",
      "Epoch 44/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.5972 - val_loss: 2.3660\n",
      "Epoch 45/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.5038 - val_loss: 2.0979\n",
      "Epoch 46/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.3873 - val_loss: 1.9294\n",
      "Epoch 47/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.2252 - val_loss: 1.9922\n",
      "Epoch 48/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.2156 - val_loss: 2.0750\n",
      "Epoch 49/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.0079 - val_loss: 1.7601\n",
      "Epoch 50/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.9822 - val_loss: 1.7117\n",
      "Epoch 51/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.9647 - val_loss: 1.7783\n",
      "Epoch 52/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.6618 - val_loss: 1.4169\n",
      "Epoch 53/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.7344 - val_loss: 1.4021\n",
      "Epoch 54/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.6235 - val_loss: 1.3525\n",
      "Epoch 55/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.4640 - val_loss: 1.1348\n",
      "Epoch 56/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.5163 - val_loss: 1.2797\n",
      "Epoch 57/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.3265 - val_loss: 1.1695\n",
      "Epoch 58/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.2928 - val_loss: 1.8019\n",
      "Epoch 59/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.2391 - val_loss: 1.1710\n",
      "Epoch 60/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.1322 - val_loss: 1.1329\n",
      "Epoch 61/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.1034 - val_loss: 1.0804\n",
      "Epoch 62/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.1163 - val_loss: 1.1241\n",
      "Epoch 63/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.1502 - val_loss: 1.0779\n",
      "Epoch 64/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.0487 - val_loss: 0.8760\n",
      "Epoch 65/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9571 - val_loss: 0.8682\n",
      "Epoch 66/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9058 - val_loss: 0.9268\n",
      "Epoch 67/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.8623\n",
      "Epoch 68/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9543 - val_loss: 0.7765\n",
      "Epoch 69/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9740 - val_loss: 0.6644\n",
      "Epoch 70/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7853 - val_loss: 0.6066\n",
      "Epoch 71/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7804 - val_loss: 0.9809\n",
      "Epoch 72/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7439 - val_loss: 0.7350\n",
      "Epoch 73/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7774 - val_loss: 0.9084\n",
      "Epoch 74/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7252 - val_loss: 0.6793\n",
      "Epoch 75/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7372 - val_loss: 0.6843\n",
      "Epoch 76/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6837 - val_loss: 0.7060\n",
      "Epoch 77/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6854 - val_loss: 0.5443\n",
      "Epoch 78/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6391 - val_loss: 0.5923\n",
      "Epoch 79/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5944 - val_loss: 0.6171\n",
      "Epoch 80/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6184 - val_loss: 0.7144\n",
      "Epoch 81/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6244 - val_loss: 0.6077\n",
      "Epoch 82/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6912 - val_loss: 0.8086\n",
      "Epoch 83/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5898 - val_loss: 0.5618\n",
      "Epoch 84/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5448 - val_loss: 0.5286\n",
      "Epoch 85/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5485 - val_loss: 0.5111\n",
      "Epoch 86/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5663 - val_loss: 0.7849\n",
      "Epoch 87/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.4847\n",
      "Epoch 88/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.6374\n",
      "Epoch 89/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5672 - val_loss: 0.5836\n",
      "Epoch 90/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5045 - val_loss: 0.4705\n",
      "Epoch 91/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5140 - val_loss: 0.4762\n",
      "Epoch 92/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5461 - val_loss: 0.4106\n",
      "Epoch 93/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5285 - val_loss: 0.6620\n",
      "Epoch 94/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7011 - val_loss: 0.7383\n",
      "Epoch 95/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.8544\n",
      "Epoch 96/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5572 - val_loss: 0.3991\n",
      "Epoch 97/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4641 - val_loss: 0.4114\n",
      "Epoch 98/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4535 - val_loss: 0.4700\n",
      "Epoch 99/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4448 - val_loss: 0.6091\n",
      "Epoch 100/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4506 - val_loss: 0.3973\n",
      "Epoch 101/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4661 - val_loss: 0.4970\n",
      "Epoch 102/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4702 - val_loss: 0.5133\n",
      "Epoch 103/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4431 - val_loss: 0.3998\n",
      "Epoch 104/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4147 - val_loss: 0.5950\n",
      "Epoch 105/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4660 - val_loss: 0.4623\n",
      "Epoch 106/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4410 - val_loss: 0.4592\n",
      "Epoch 107/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4385 - val_loss: 0.5836\n",
      "Epoch 108/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4532 - val_loss: 0.7291\n",
      "Epoch 109/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4397 - val_loss: 0.4873\n",
      "Epoch 110/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4924 - val_loss: 0.3892\n",
      "Epoch 111/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4748 - val_loss: 0.4660\n",
      "Epoch 112/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3854 - val_loss: 0.3446\n",
      "Epoch 113/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5425 - val_loss: 0.4892\n",
      "Epoch 114/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4568 - val_loss: 0.3046\n",
      "Epoch 115/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3571 - val_loss: 0.3412\n",
      "Epoch 116/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4081 - val_loss: 0.5015\n",
      "Epoch 117/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4492 - val_loss: 0.3574\n",
      "Epoch 118/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4232 - val_loss: 0.3628\n",
      "Epoch 119/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3561 - val_loss: 0.3385\n",
      "Epoch 120/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3325 - val_loss: 0.3284\n",
      "Epoch 121/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4327 - val_loss: 0.3487\n",
      "Epoch 122/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4428 - val_loss: 0.6436\n",
      "Epoch 123/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3789 - val_loss: 0.2684\n",
      "Epoch 124/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3394 - val_loss: 0.2925\n",
      "Epoch 125/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3513\n",
      "Epoch 126/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3332 - val_loss: 0.3187\n",
      "Epoch 127/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4584 - val_loss: 0.2857\n",
      "Epoch 128/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3247 - val_loss: 0.2719\n",
      "Epoch 129/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3869 - val_loss: 0.3398\n",
      "Epoch 130/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3571 - val_loss: 0.5171\n",
      "Epoch 131/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3291 - val_loss: 0.2726\n",
      "Epoch 132/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3233 - val_loss: 0.3315\n",
      "Epoch 133/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3114\n",
      "Epoch 134/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3156 - val_loss: 0.2918\n",
      "Epoch 135/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3167 - val_loss: 0.4296\n",
      "Epoch 136/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3499 - val_loss: 0.3808\n",
      "Epoch 137/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3632 - val_loss: 0.2974\n",
      "Epoch 138/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3403 - val_loss: 0.2440\n",
      "Epoch 139/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3128 - val_loss: 0.4297\n",
      "Epoch 140/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3354 - val_loss: 0.3041\n",
      "Epoch 141/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2864 - val_loss: 0.2509\n",
      "Epoch 142/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3240 - val_loss: 0.2791\n",
      "Epoch 143/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3017 - val_loss: 0.3030\n",
      "Epoch 144/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2902 - val_loss: 0.2500\n",
      "Epoch 145/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2671 - val_loss: 0.2914\n",
      "Epoch 146/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2970 - val_loss: 0.3778\n",
      "Epoch 147/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3164 - val_loss: 0.3291\n",
      "Epoch 148/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3506 - val_loss: 0.4297\n",
      "Epoch 149/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3791 - val_loss: 0.2875\n",
      "Epoch 150/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2642 - val_loss: 0.3150\n",
      "Epoch 151/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3460 - val_loss: 0.5130\n",
      "Epoch 152/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 0.2403\n",
      "Epoch 153/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2848 - val_loss: 0.2903\n",
      "Epoch 154/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2990 - val_loss: 0.2578\n",
      "Epoch 155/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3312 - val_loss: 0.2671\n",
      "Epoch 156/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2806 - val_loss: 0.3298\n",
      "Epoch 157/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3009 - val_loss: 0.2312\n",
      "Epoch 158/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2592 - val_loss: 0.2664\n",
      "Epoch 159/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2728 - val_loss: 0.2600\n",
      "Epoch 160/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2288 - val_loss: 0.2125\n",
      "Epoch 161/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2379 - val_loss: 0.2256\n",
      "Epoch 162/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2400 - val_loss: 0.2733\n",
      "Epoch 163/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2521 - val_loss: 0.2710\n",
      "Epoch 164/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2788 - val_loss: 0.3645\n",
      "Epoch 165/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2843 - val_loss: 0.3124\n",
      "Epoch 166/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2776 - val_loss: 0.2159\n",
      "Epoch 167/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2404 - val_loss: 0.1846\n",
      "Epoch 168/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2712 - val_loss: 0.3008\n",
      "Epoch 169/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2637 - val_loss: 0.2774\n",
      "Epoch 170/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2397 - val_loss: 0.2043\n",
      "Epoch 171/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2340 - val_loss: 0.2630\n",
      "Epoch 172/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2273 - val_loss: 0.2017\n",
      "Epoch 173/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2368 - val_loss: 0.2624\n",
      "Epoch 174/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2463 - val_loss: 0.3236\n",
      "Epoch 175/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2572 - val_loss: 0.2474\n",
      "Epoch 176/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2008 - val_loss: 0.1682\n",
      "Epoch 177/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1947 - val_loss: 0.2926\n",
      "Epoch 178/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2038 - val_loss: 0.2020\n",
      "Epoch 179/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2197 - val_loss: 0.1739\n",
      "Epoch 180/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2163 - val_loss: 0.2297\n",
      "Epoch 181/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2224 - val_loss: 0.4051\n",
      "Epoch 182/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2033 - val_loss: 0.2091\n",
      "Epoch 183/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 0.2629\n",
      "Epoch 184/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3171 - val_loss: 0.1969\n",
      "Epoch 185/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2167 - val_loss: 0.3573\n",
      "Epoch 186/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1996 - val_loss: 0.1854\n",
      "Epoch 187/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2045 - val_loss: 0.1859\n",
      "Epoch 188/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2154 - val_loss: 0.1996\n",
      "Epoch 189/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1913 - val_loss: 0.1789\n",
      "Epoch 190/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2157 - val_loss: 0.1882\n",
      "Epoch 191/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1848 - val_loss: 0.1612\n",
      "Epoch 192/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 0.3485\n",
      "Epoch 193/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2194 - val_loss: 0.2237\n",
      "Epoch 194/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2114 - val_loss: 0.2480\n",
      "Epoch 195/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1913 - val_loss: 0.1768\n",
      "Epoch 196/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2658 - val_loss: 0.1981\n",
      "Epoch 197/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2094 - val_loss: 0.1837\n",
      "Epoch 198/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1928 - val_loss: 0.2373\n",
      "Epoch 199/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1764 - val_loss: 0.2109\n",
      "Epoch 200/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2082 - val_loss: 0.1891\n",
      "Epoch 201/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2035 - val_loss: 0.2019\n",
      "Epoch 202/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2179 - val_loss: 0.2175\n",
      "Epoch 203/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1774 - val_loss: 0.2118\n",
      "Epoch 204/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1742 - val_loss: 0.2684\n",
      "Epoch 205/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1735 - val_loss: 0.2039\n",
      "Epoch 206/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2221 - val_loss: 0.1557\n",
      "Epoch 207/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1649 - val_loss: 0.2156\n",
      "Epoch 208/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1733 - val_loss: 0.1340\n",
      "Epoch 209/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2202 - val_loss: 0.1839\n",
      "Epoch 210/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2560 - val_loss: 0.1457\n",
      "Epoch 211/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1768 - val_loss: 0.1339\n",
      "Epoch 212/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1633 - val_loss: 0.2997\n",
      "Epoch 213/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1736 - val_loss: 0.1396\n",
      "Epoch 214/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1699 - val_loss: 0.1332\n",
      "Epoch 215/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1585 - val_loss: 0.1489\n",
      "Epoch 216/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2083 - val_loss: 0.2368\n",
      "Epoch 217/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1847 - val_loss: 0.1344\n",
      "Epoch 218/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1804 - val_loss: 0.1997\n",
      "Epoch 219/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1712 - val_loss: 0.1542\n",
      "Epoch 220/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1737 - val_loss: 0.1483\n",
      "Epoch 221/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2009 - val_loss: 0.2336\n",
      "Epoch 222/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1875 - val_loss: 0.1616\n",
      "Epoch 223/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1595 - val_loss: 0.1410\n",
      "Epoch 224/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2084 - val_loss: 0.2019\n",
      "Epoch 225/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2289 - val_loss: 0.1633\n",
      "Epoch 226/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1490 - val_loss: 0.1408\n",
      "Epoch 227/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1713 - val_loss: 0.1606\n",
      "Epoch 228/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1767 - val_loss: 0.1487\n",
      "Epoch 229/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 0.1404\n",
      "Epoch 230/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1408 - val_loss: 0.1933\n",
      "Epoch 231/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1532 - val_loss: 0.1616\n",
      "Epoch 232/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.1494\n",
      "Epoch 233/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1797 - val_loss: 0.1758\n",
      "Epoch 234/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 0.1649\n",
      "Epoch 235/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1675 - val_loss: 0.2197\n",
      "Epoch 236/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1321 - val_loss: 0.1664\n",
      "Epoch 237/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1430 - val_loss: 0.1326\n",
      "Epoch 238/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1403 - val_loss: 0.1214\n",
      "Epoch 239/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1689 - val_loss: 0.1968\n",
      "Epoch 240/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1634 - val_loss: 0.1547\n",
      "Epoch 241/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1520 - val_loss: 0.1757\n",
      "Epoch 242/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1428 - val_loss: 0.1233\n",
      "Epoch 243/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1917 - val_loss: 0.1234\n",
      "Epoch 244/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1898 - val_loss: 0.1429\n",
      "Epoch 245/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1828 - val_loss: 0.1464\n",
      "Epoch 246/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1418 - val_loss: 0.1470\n",
      "Epoch 247/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1689 - val_loss: 0.3365\n",
      "Epoch 248/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1711 - val_loss: 0.1751\n",
      "Epoch 249/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1445 - val_loss: 0.1372\n",
      "Epoch 250/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1246 - val_loss: 0.1360\n",
      "Epoch 251/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 0.1253\n",
      "Epoch 252/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 0.1850\n",
      "Epoch 253/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.1284\n",
      "Epoch 254/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1638 - val_loss: 0.1552\n",
      "Epoch 255/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2109 - val_loss: 0.3710\n",
      "Epoch 256/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1637 - val_loss: 0.2009\n",
      "Epoch 257/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1420 - val_loss: 0.1194\n",
      "Epoch 258/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1374 - val_loss: 0.1681\n",
      "Epoch 259/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1279 - val_loss: 0.1258\n",
      "Epoch 260/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1251 - val_loss: 0.1366\n",
      "Epoch 261/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1221 - val_loss: 0.1628\n",
      "Epoch 262/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.1135\n",
      "Epoch 263/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1197 - val_loss: 0.1339\n",
      "Epoch 264/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1389 - val_loss: 0.1185\n",
      "Epoch 265/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 0.1340\n",
      "Epoch 266/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1195 - val_loss: 0.1279\n",
      "Epoch 267/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1471 - val_loss: 0.2529\n",
      "Epoch 268/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1653 - val_loss: 0.1244\n",
      "Epoch 269/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.1203\n",
      "Epoch 270/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1325 - val_loss: 0.1677\n",
      "Epoch 271/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1331 - val_loss: 0.1147\n",
      "Epoch 272/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1146 - val_loss: 0.1446\n",
      "Epoch 273/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1105 - val_loss: 0.1696\n",
      "Epoch 274/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1251 - val_loss: 0.1491\n",
      "Epoch 275/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1135 - val_loss: 0.1181\n",
      "Epoch 276/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1161\n",
      "Epoch 277/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1866 - val_loss: 0.1105\n",
      "Epoch 278/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 0.1288\n",
      "Epoch 279/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2098 - val_loss: 0.1525\n",
      "Epoch 280/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1143 - val_loss: 0.1404\n",
      "Epoch 281/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1449 - val_loss: 0.1108\n",
      "Epoch 282/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 0.0997\n",
      "Epoch 283/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1892 - val_loss: 0.1500\n",
      "Epoch 284/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.1132\n",
      "Epoch 285/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1060 - val_loss: 0.1039\n",
      "Epoch 286/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1145 - val_loss: 0.2430\n",
      "Epoch 287/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1192 - val_loss: 0.1494\n",
      "Epoch 288/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 0.1304\n",
      "Epoch 289/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1162 - val_loss: 0.0956\n",
      "Epoch 290/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1414 - val_loss: 0.1429\n",
      "Epoch 291/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1599 - val_loss: 0.1361\n",
      "Epoch 292/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1501 - val_loss: 0.2776\n",
      "Epoch 293/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1229 - val_loss: 0.1477\n",
      "Epoch 294/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.0975\n",
      "Epoch 295/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1012 - val_loss: 0.0915\n",
      "Epoch 296/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1137 - val_loss: 0.1040\n",
      "Epoch 297/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1384 - val_loss: 0.1349\n",
      "Epoch 298/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1226 - val_loss: 0.1058\n",
      "Epoch 299/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 0.1303\n",
      "Epoch 300/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 0.1814\n",
      "Epoch 301/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1467 - val_loss: 0.1496\n",
      "Epoch 302/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1064 - val_loss: 0.1011\n",
      "Epoch 303/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.1047\n",
      "Epoch 304/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1038 - val_loss: 0.1172\n",
      "Epoch 305/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.1193\n",
      "Epoch 306/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.1727\n",
      "Epoch 307/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1082 - val_loss: 0.0866\n",
      "Epoch 308/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1226 - val_loss: 0.1341\n",
      "Epoch 309/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1569 - val_loss: 0.0985\n",
      "Epoch 310/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1189 - val_loss: 0.1155\n",
      "Epoch 311/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1172 - val_loss: 0.1700\n",
      "Epoch 312/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1311 - val_loss: 0.1303\n",
      "Epoch 313/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1252 - val_loss: 0.1394\n",
      "Epoch 314/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1094 - val_loss: 0.1346\n",
      "Epoch 315/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0941 - val_loss: 0.1000\n",
      "Epoch 316/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 0.0798\n",
      "Epoch 317/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0945 - val_loss: 0.1231\n",
      "Epoch 318/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1450 - val_loss: 0.1529\n",
      "Epoch 319/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1183 - val_loss: 0.1254\n",
      "Epoch 320/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2478 - val_loss: 0.1387\n",
      "Epoch 321/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1405 - val_loss: 0.1396\n",
      "Epoch 322/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1123 - val_loss: 0.1986\n",
      "Epoch 323/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1049 - val_loss: 0.1057\n",
      "Epoch 324/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1000 - val_loss: 0.0973\n",
      "Epoch 325/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 0.1071\n",
      "Epoch 326/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.2467\n",
      "Epoch 327/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.1902\n",
      "Epoch 328/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.1097\n",
      "Epoch 329/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.1323\n",
      "Epoch 330/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1097 - val_loss: 0.1505\n",
      "Epoch 331/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.1151\n",
      "Epoch 332/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0943 - val_loss: 0.0827\n",
      "Epoch 333/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1046 - val_loss: 0.1587\n",
      "Epoch 334/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.1181\n",
      "Epoch 335/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2005 - val_loss: 0.3176\n",
      "Epoch 336/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1299 - val_loss: 0.1321\n",
      "Epoch 337/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 0.1109\n",
      "Epoch 338/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1803 - val_loss: 0.2277\n",
      "Epoch 339/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1104 - val_loss: 0.1073\n",
      "Epoch 340/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1409 - val_loss: 0.1720\n",
      "Epoch 341/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 0.0996\n",
      "Epoch 342/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1186 - val_loss: 0.2349\n",
      "Epoch 343/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1237 - val_loss: 0.0916\n",
      "Epoch 344/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.0846\n",
      "Epoch 345/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.0914\n",
      "Epoch 346/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1347 - val_loss: 0.2747\n",
      "Epoch 347/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1164 - val_loss: 0.0773\n",
      "Epoch 348/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.1282\n",
      "Epoch 349/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1054 - val_loss: 0.1012\n",
      "Epoch 350/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.1314\n",
      "Epoch 351/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 0.0924\n",
      "Epoch 352/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0885 - val_loss: 0.0881\n",
      "Epoch 353/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2125 - val_loss: 0.1446\n",
      "Epoch 354/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.1338\n",
      "Epoch 355/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1182 - val_loss: 0.1092\n",
      "Epoch 356/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1090 - val_loss: 0.1076\n",
      "Epoch 357/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0849 - val_loss: 0.1180\n",
      "Epoch 358/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0867 - val_loss: 0.0979\n",
      "Epoch 359/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0968 - val_loss: 0.0917\n",
      "Epoch 360/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1448 - val_loss: 0.2292\n",
      "Epoch 361/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1037 - val_loss: 0.0792\n",
      "Epoch 362/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0999 - val_loss: 0.1544\n",
      "Epoch 363/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1029 - val_loss: 0.0912\n",
      "Epoch 364/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1287 - val_loss: 0.1791\n",
      "Epoch 365/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 0.0905\n",
      "Epoch 366/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1322 - val_loss: 0.1328\n",
      "Epoch 367/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1043 - val_loss: 0.2649\n",
      "Epoch 368/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1130 - val_loss: 0.1781\n",
      "Epoch 369/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1074 - val_loss: 0.1950\n",
      "Epoch 370/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0900 - val_loss: 0.0871\n",
      "Epoch 371/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1143 - val_loss: 0.0932\n",
      "Epoch 372/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.1426\n",
      "Epoch 373/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0986 - val_loss: 0.2643\n",
      "Epoch 374/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.1567\n",
      "Epoch 375/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1006 - val_loss: 0.1198\n",
      "Epoch 376/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1427 - val_loss: 0.1645\n",
      "Epoch 377/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1088 - val_loss: 0.1302\n",
      "Epoch 378/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0972 - val_loss: 0.1136\n",
      "Epoch 379/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1051 - val_loss: 0.0810\n",
      "Epoch 380/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0933 - val_loss: 0.0966\n",
      "Epoch 381/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0829 - val_loss: 0.0757\n",
      "Epoch 382/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0776 - val_loss: 0.1040\n",
      "Epoch 383/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 0.0968\n",
      "Epoch 384/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0853 - val_loss: 0.1058\n",
      "Epoch 385/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 0.0986\n",
      "Epoch 386/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1407 - val_loss: 0.1176\n",
      "Epoch 387/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0971 - val_loss: 0.1002\n",
      "Epoch 388/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0861 - val_loss: 0.1231\n",
      "Epoch 389/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0945 - val_loss: 0.0787\n",
      "Epoch 390/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1224 - val_loss: 0.1043\n",
      "Epoch 391/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0788 - val_loss: 0.1148\n",
      "Epoch 392/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0869 - val_loss: 0.0838\n",
      "Epoch 393/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0925 - val_loss: 0.0846\n",
      "Epoch 394/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0934 - val_loss: 0.1261\n",
      "Epoch 395/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1382 - val_loss: 0.2408\n",
      "Epoch 396/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1258 - val_loss: 0.0966\n",
      "Epoch 397/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0759 - val_loss: 0.0842\n",
      "Epoch 398/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0900 - val_loss: 0.0950\n",
      "Epoch 399/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0779 - val_loss: 0.0944\n",
      "Epoch 400/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1010 - val_loss: 0.1309\n",
      "Epoch 401/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1207 - val_loss: 0.1208\n",
      "Epoch 402/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.1067 - val_loss: 0.1087\n",
      "Epoch 403/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 0.0935\n",
      "Epoch 404/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0897 - val_loss: 0.0824\n",
      "Epoch 405/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1000 - val_loss: 0.1763\n",
      "Epoch 406/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1123 - val_loss: 0.1180\n",
      "Epoch 407/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0772 - val_loss: 0.0842\n",
      "Epoch 408/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0956 - val_loss: 0.1063\n",
      "Epoch 409/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.1363 - val_loss: 0.0960\n",
      "Epoch 410/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0764 - val_loss: 0.0888\n",
      "Epoch 411/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 0.0914\n",
      "Epoch 412/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0794 - val_loss: 0.1221\n",
      "Epoch 413/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0805 - val_loss: 0.0777\n",
      "Epoch 414/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0817 - val_loss: 0.1056\n",
      "Epoch 415/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.0835 - val_loss: 0.0945\n",
      "Epoch 416/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.1352\n",
      "Epoch 417/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 0.2450\n",
      "Epoch 418/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1835 - val_loss: 0.0967\n",
      "Epoch 419/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0778 - val_loss: 0.0769\n",
      "Epoch 420/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0938 - val_loss: 0.1311\n",
      "Epoch 421/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.1226\n",
      "Epoch 422/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.0924 - val_loss: 0.0783\n",
      "Epoch 423/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0864 - val_loss: 0.0795\n",
      "Epoch 424/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0637 - val_loss: 0.0894\n",
      "Epoch 425/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0863 - val_loss: 0.1081\n",
      "Epoch 426/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1042 - val_loss: 0.2390\n",
      "Epoch 427/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.1231 - val_loss: 0.1129\n",
      "Epoch 428/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.1004 - val_loss: 0.2130\n",
      "Epoch 429/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1169 - val_loss: 0.0863\n",
      "Epoch 430/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1019 - val_loss: 0.1337\n",
      "Epoch 431/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 0.1644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19f26a88b50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_valid,y_valid), \n",
    "          epochs=500, \n",
    "          batch_size=25,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(x_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.0, -3.0, 10.0, 12.5, 0.0, 9.0] => 21.352869 (expected 21.175830)\n",
      "[14.0, -4.0, 10.0, 50.0, 0.0, 9.0] => 27.349899 (expected 27.502471)\n",
      "[2.0, 3.0, 100.0, 25.0, 12.5, 9.0] => 23.891647 (expected 23.682655)\n",
      "[18.0, 0.0, 100.0, 25.0, 0.0, 15.0] => 14.027429 (expected 13.047312)\n",
      "[2.0, -1.0, 50.0, 25.0, 0.0, 3.0] => 30.417183 (expected 30.132099)\n",
      "[18.0, -4.0, 100.0, 12.5, 0.0, 9.0] => 14.248030 (expected 14.090038)\n",
      "[13.0, 2.0, 50.0, 50.0, 0.0, 9.0] => 19.174734 (expected 19.110903)\n",
      "[19.0, -3.0, 100.0, 25.0, 0.0, 3.0] => 12.813681 (expected 12.820579)\n",
      "[15.0, 3.0, 10.0, 50.0, 0.0, 9.0] => 20.671587 (expected 20.500554)\n",
      "[18.0, -1.0, 50.0, 25.0, 0.0, 3.0] => 18.845303 (expected 19.519504)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('%s => %f (expected %f)' % (x_test[i].tolist(), predictions[i], y_test[i] ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate difference between actual OSNIR and predicted OSNIR (in dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_difference = np.zeros(599)\n",
    "for i in range(599):\n",
    "    pred_difference[i] = y_test[i] - predictions[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Difference is approximately 0.314 dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.191778449263915"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(pred_difference[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Actual OSNIR (dB)\" : y_test.tolist(), \"Predicted OSNIR (dB)\" : predictions.tolist(), \"Difference (dB)\" : pred_difference.tolist()})\n",
    "pred_df.to_csv(\"OSNIR_Prediction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0.,   1.,   0.,   3.,   6.,  23., 111.,\n",
       "        160., 180., 101.,  12.,   0.,   1.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.]),\n",
       " array([-4.  , -3.68, -3.36, -3.04, -2.72, -2.4 , -2.08, -1.76, -1.44,\n",
       "        -1.12, -0.8 , -0.48, -0.16,  0.16,  0.48,  0.8 ,  1.12,  1.44,\n",
       "         1.76,  2.08,  2.4 ,  2.72,  3.04,  3.36,  3.68,  4.  ]),\n",
       " <BarContainer object of 25 artists>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Deep Neural Network OSNIR Mismatch')\n",
    "plt.xlabel('Mismatch (dB)')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.hist(pred_difference, range=(-4,4), rwidth=0.9, bins=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd7dbc09f62c6934dc245b76251cec5fd949cd1cb8bcb775af9208cac5a10da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
