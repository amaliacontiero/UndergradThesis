{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import OSNIR Dataset and shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OSNIR dataset\n",
    "df = pd.read_csv('../Data/OSNIR_values_extendedv3_new datasetbcsv.csv')\n",
    "# shuffling rows of OSNIR dataframe and reset indexes\n",
    "df = df.sample(frac=1)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview of first 5 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OSNIRnumerical(dB)</th>\n",
       "      <th>Ns</th>\n",
       "      <th>Pch(dBm)</th>\n",
       "      <th>L(km)</th>\n",
       "      <th>B(GHz)</th>\n",
       "      <th>GB(GHz)</th>\n",
       "      <th>Nch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.655224</td>\n",
       "      <td>38</td>\n",
       "      <td>-3</td>\n",
       "      <td>100</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.920383</td>\n",
       "      <td>50</td>\n",
       "      <td>-7</td>\n",
       "      <td>50</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.138504</td>\n",
       "      <td>30</td>\n",
       "      <td>-2</td>\n",
       "      <td>50</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.459131</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.856131</td>\n",
       "      <td>19</td>\n",
       "      <td>-8</td>\n",
       "      <td>10</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OSNIRnumerical(dB)  Ns  Pch(dBm)  L(km)  B(GHz)  GB(GHz)  Nch\n",
       "0           10.655224  38        -3    100    12.5      0.0    9\n",
       "1           14.920383  50        -7     50    25.0      0.0    3\n",
       "2           18.138504  30        -2     50    25.0      0.0    3\n",
       "3            9.459131  36         1    100    25.0      0.0    9\n",
       "4           25.856131  19        -8     10    25.0      0.0    9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1800 values for training (0,1800-1)\\\n",
    "600 values for validation (1800,2400-1)\\\n",
    "600 values for testing (2400,3000-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input x aka Ns, Pch, L, B, GB, Nch values\n",
    "x_train = data[0:1800-1, 1:7]\n",
    "x_valid = data[1800:2400-1, 1:7]\n",
    "x_test = data[2400:3000-1, 1:7]\n",
    "\n",
    "# output y aka OSNIR values\n",
    "y_train = data[0:1800-1, 0]\n",
    "y_valid = data[1800:2400-1, 0]\n",
    "y_test = data[2400:3000-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 38. ,  -3. , 100. ,  12.5,   0. ,   9. ],\n",
       "       [ 50. ,  -7. ,  50. ,  25. ,   0. ,   3. ],\n",
       "       [ 30. ,  -2. ,  50. ,  25. ,   0. ,   3. ],\n",
       "       ...,\n",
       "       [ 48. ,   0. , 100. ,  12.5,   0. ,   9. ],\n",
       "       [ 41. ,   3. ,  50. ,  25. ,  25. ,   9. ],\n",
       "       [ 22. ,  -3. ,  50. ,  12.5,   0. ,   9. ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Neural Network\n",
    "1 Input layer, 2 hidden layers (32 neurons each) and 1 Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=(6,), activation='relu'),  # first hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # second hidden layer\n",
    "    keras.layers.Dense(1, activation='relu') # output layer (3)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early hault of training if loss has not improved in 50 epochs in validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Optimizer, Mean Square Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse'\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model, run for 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "72/72 [==============================] - 1s 3ms/step - loss: 105.8782 - val_loss: 54.2099\n",
      "Epoch 2/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 46.0105 - val_loss: 38.0755\n",
      "Epoch 3/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 35.1541 - val_loss: 32.8401\n",
      "Epoch 4/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 29.8212 - val_loss: 29.3665\n",
      "Epoch 5/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 27.1150 - val_loss: 28.1113\n",
      "Epoch 6/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 25.2967 - val_loss: 26.8154\n",
      "Epoch 7/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 23.1451 - val_loss: 25.6526\n",
      "Epoch 8/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 21.4167 - val_loss: 23.4821\n",
      "Epoch 9/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 20.1709 - val_loss: 23.1239\n",
      "Epoch 10/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 18.5513 - val_loss: 20.4955\n",
      "Epoch 11/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 17.3067 - val_loss: 20.8362\n",
      "Epoch 12/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 16.4618 - val_loss: 17.8848\n",
      "Epoch 13/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 15.6467 - val_loss: 16.8935\n",
      "Epoch 14/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 14.4134 - val_loss: 15.9829\n",
      "Epoch 15/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 13.6066 - val_loss: 15.0908\n",
      "Epoch 16/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 12.4524 - val_loss: 14.2760\n",
      "Epoch 17/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 11.7207 - val_loss: 13.7342\n",
      "Epoch 18/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 10.9614 - val_loss: 12.8323\n",
      "Epoch 19/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 10.4605 - val_loss: 11.5776\n",
      "Epoch 20/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 9.6505 - val_loss: 11.6270\n",
      "Epoch 21/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 9.1173 - val_loss: 10.4708\n",
      "Epoch 22/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8.8350 - val_loss: 10.5682\n",
      "Epoch 23/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7.8372 - val_loss: 9.0347\n",
      "Epoch 24/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7.4277 - val_loss: 8.2300\n",
      "Epoch 25/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.8693 - val_loss: 8.3755\n",
      "Epoch 26/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.2595 - val_loss: 7.8951\n",
      "Epoch 27/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.7349 - val_loss: 7.1523\n",
      "Epoch 28/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.5775 - val_loss: 6.6887\n",
      "Epoch 29/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.1312 - val_loss: 5.4778\n",
      "Epoch 30/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.6886 - val_loss: 5.0568\n",
      "Epoch 31/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.3618 - val_loss: 4.7054\n",
      "Epoch 32/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.1153 - val_loss: 4.5859\n",
      "Epoch 33/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.6413 - val_loss: 4.1310\n",
      "Epoch 34/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.5174 - val_loss: 3.6683\n",
      "Epoch 35/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.2328 - val_loss: 4.1308\n",
      "Epoch 36/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.0959 - val_loss: 3.2721\n",
      "Epoch 37/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.7947 - val_loss: 3.3572\n",
      "Epoch 38/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.6101 - val_loss: 3.3265\n",
      "Epoch 39/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.4180 - val_loss: 3.5154\n",
      "Epoch 40/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.3775 - val_loss: 3.2713\n",
      "Epoch 41/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.2918 - val_loss: 2.5820\n",
      "Epoch 42/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.1571 - val_loss: 2.5855\n",
      "Epoch 43/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.0670 - val_loss: 2.2384\n",
      "Epoch 44/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.8586 - val_loss: 2.3771\n",
      "Epoch 45/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.0078 - val_loss: 2.0352\n",
      "Epoch 46/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.7688 - val_loss: 2.1707\n",
      "Epoch 47/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.6047 - val_loss: 1.7820\n",
      "Epoch 48/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5906 - val_loss: 1.8338\n",
      "Epoch 49/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.4168 - val_loss: 2.3796\n",
      "Epoch 50/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5621 - val_loss: 1.7741\n",
      "Epoch 51/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5076 - val_loss: 1.4709\n",
      "Epoch 52/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.3315 - val_loss: 1.5357\n",
      "Epoch 53/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.2323 - val_loss: 1.3746\n",
      "Epoch 54/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2444 - val_loss: 1.3722\n",
      "Epoch 55/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2161 - val_loss: 1.3650\n",
      "Epoch 56/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1930 - val_loss: 1.3947\n",
      "Epoch 57/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1736 - val_loss: 1.2424\n",
      "Epoch 58/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2783 - val_loss: 1.1760\n",
      "Epoch 59/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.1502 - val_loss: 1.2192\n",
      "Epoch 60/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.0686 - val_loss: 1.0709\n",
      "Epoch 61/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.3129 - val_loss: 1.1581\n",
      "Epoch 62/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.0252 - val_loss: 1.2838\n",
      "Epoch 63/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.0379 - val_loss: 0.9824\n",
      "Epoch 64/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9224 - val_loss: 1.1661\n",
      "Epoch 65/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9767\n",
      "Epoch 66/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9504 - val_loss: 0.9971\n",
      "Epoch 67/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8902 - val_loss: 0.9685\n",
      "Epoch 68/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8533 - val_loss: 0.8137\n",
      "Epoch 69/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8449 - val_loss: 0.7909\n",
      "Epoch 70/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8753 - val_loss: 0.9472\n",
      "Epoch 71/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.9298 - val_loss: 0.8594\n",
      "Epoch 72/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7989 - val_loss: 0.7021\n",
      "Epoch 73/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7033 - val_loss: 0.7053\n",
      "Epoch 74/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6520 - val_loss: 0.6290\n",
      "Epoch 75/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7038 - val_loss: 1.0705\n",
      "Epoch 76/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6773 - val_loss: 0.6772\n",
      "Epoch 77/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6638 - val_loss: 0.5792\n",
      "Epoch 78/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7064 - val_loss: 0.6203\n",
      "Epoch 79/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6187 - val_loss: 0.6828\n",
      "Epoch 80/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6788 - val_loss: 0.5980\n",
      "Epoch 81/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6744 - val_loss: 0.6961\n",
      "Epoch 82/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6101 - val_loss: 0.5578\n",
      "Epoch 83/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7103 - val_loss: 0.8459\n",
      "Epoch 84/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6181 - val_loss: 0.5045\n",
      "Epoch 85/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5559 - val_loss: 0.6661\n",
      "Epoch 86/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.7698\n",
      "Epoch 87/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6301 - val_loss: 0.4513\n",
      "Epoch 88/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4549 - val_loss: 0.4477\n",
      "Epoch 89/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5204 - val_loss: 0.4139\n",
      "Epoch 90/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4866 - val_loss: 0.4286\n",
      "Epoch 91/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4534 - val_loss: 0.4257\n",
      "Epoch 92/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4589 - val_loss: 0.4598\n",
      "Epoch 93/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4429 - val_loss: 0.5905\n",
      "Epoch 94/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4693 - val_loss: 0.4701\n",
      "Epoch 95/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4762 - val_loss: 0.4194\n",
      "Epoch 96/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4468 - val_loss: 0.4982\n",
      "Epoch 97/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3842 - val_loss: 0.4478\n",
      "Epoch 98/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.3637 - val_loss: 0.4631\n",
      "Epoch 99/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6054 - val_loss: 0.3288\n",
      "Epoch 100/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4019 - val_loss: 0.5093\n",
      "Epoch 101/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4814 - val_loss: 0.4446\n",
      "Epoch 102/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4386 - val_loss: 0.3497\n",
      "Epoch 103/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.3842 - val_loss: 0.3560\n",
      "Epoch 104/500\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.4144 - val_loss: 0.4787\n",
      "Epoch 105/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4307 - val_loss: 0.3290\n",
      "Epoch 106/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.3622 - val_loss: 0.3220\n",
      "Epoch 107/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4044 - val_loss: 0.3846\n",
      "Epoch 108/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3982 - val_loss: 0.4786\n",
      "Epoch 109/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3360 - val_loss: 0.4175\n",
      "Epoch 110/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3828 - val_loss: 0.3392\n",
      "Epoch 111/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3533 - val_loss: 0.2936\n",
      "Epoch 112/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3565 - val_loss: 0.3897\n",
      "Epoch 113/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.2859\n",
      "Epoch 114/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3164 - val_loss: 0.2830\n",
      "Epoch 115/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.5967\n",
      "Epoch 116/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3781 - val_loss: 0.2806\n",
      "Epoch 117/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3635 - val_loss: 0.4349\n",
      "Epoch 118/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3795 - val_loss: 0.3433\n",
      "Epoch 119/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3492 - val_loss: 0.2904\n",
      "Epoch 120/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3434 - val_loss: 0.2832\n",
      "Epoch 121/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3204 - val_loss: 0.2980\n",
      "Epoch 122/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3119 - val_loss: 0.3373\n",
      "Epoch 123/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3113 - val_loss: 0.3008\n",
      "Epoch 124/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3245 - val_loss: 0.2387\n",
      "Epoch 125/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3512 - val_loss: 0.4497\n",
      "Epoch 126/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3882 - val_loss: 0.3377\n",
      "Epoch 127/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3265 - val_loss: 0.2815\n",
      "Epoch 128/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3111 - val_loss: 0.3277\n",
      "Epoch 129/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3160 - val_loss: 0.2728\n",
      "Epoch 130/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3068 - val_loss: 0.2530\n",
      "Epoch 131/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3522 - val_loss: 0.2531\n",
      "Epoch 132/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2851 - val_loss: 0.2987\n",
      "Epoch 133/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3075 - val_loss: 0.3398\n",
      "Epoch 134/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3271 - val_loss: 0.2653\n",
      "Epoch 135/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3343 - val_loss: 0.2598\n",
      "Epoch 136/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2936 - val_loss: 0.3107\n",
      "Epoch 137/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3443 - val_loss: 0.2514\n",
      "Epoch 138/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2956 - val_loss: 0.2452\n",
      "Epoch 139/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2666 - val_loss: 0.2422\n",
      "Epoch 140/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3450 - val_loss: 0.2851\n",
      "Epoch 141/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2979 - val_loss: 0.2653\n",
      "Epoch 142/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3259 - val_loss: 0.2646\n",
      "Epoch 143/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2734 - val_loss: 0.2378\n",
      "Epoch 144/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3066 - val_loss: 0.2801\n",
      "Epoch 145/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3343 - val_loss: 0.2298\n",
      "Epoch 146/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3260 - val_loss: 0.2507\n",
      "Epoch 147/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2968 - val_loss: 0.4925\n",
      "Epoch 148/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2885 - val_loss: 0.2242\n",
      "Epoch 149/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2558 - val_loss: 0.4753\n",
      "Epoch 150/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2799 - val_loss: 0.2569\n",
      "Epoch 151/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2885 - val_loss: 0.2732\n",
      "Epoch 152/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2422 - val_loss: 0.2373\n",
      "Epoch 153/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2657 - val_loss: 0.3171\n",
      "Epoch 154/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2648 - val_loss: 0.2970\n",
      "Epoch 155/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2524 - val_loss: 0.2072\n",
      "Epoch 156/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2502 - val_loss: 0.2629\n",
      "Epoch 157/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3407 - val_loss: 0.5115\n",
      "Epoch 158/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2728 - val_loss: 0.2917\n",
      "Epoch 159/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2683 - val_loss: 0.4290\n",
      "Epoch 160/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2622 - val_loss: 0.2538\n",
      "Epoch 161/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2575 - val_loss: 0.2414\n",
      "Epoch 162/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2602 - val_loss: 0.3576\n",
      "Epoch 163/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2684 - val_loss: 0.3058\n",
      "Epoch 164/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2505 - val_loss: 0.3351\n",
      "Epoch 165/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2970 - val_loss: 0.2336\n",
      "Epoch 166/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2420 - val_loss: 0.2109\n",
      "Epoch 167/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2243 - val_loss: 0.4213\n",
      "Epoch 168/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2952 - val_loss: 0.3224\n",
      "Epoch 169/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2597 - val_loss: 0.2626\n",
      "Epoch 170/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2972 - val_loss: 0.2379\n",
      "Epoch 171/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3166 - val_loss: 0.3157\n",
      "Epoch 172/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2554 - val_loss: 0.2495\n",
      "Epoch 173/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2403 - val_loss: 0.2634\n",
      "Epoch 174/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2543 - val_loss: 0.3927\n",
      "Epoch 175/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2443 - val_loss: 0.2022\n",
      "Epoch 176/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2356 - val_loss: 0.3203\n",
      "Epoch 177/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3122 - val_loss: 0.4528\n",
      "Epoch 178/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.2828\n",
      "Epoch 179/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2271 - val_loss: 0.3776\n",
      "Epoch 180/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2465 - val_loss: 0.2168\n",
      "Epoch 181/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2103 - val_loss: 0.2169\n",
      "Epoch 182/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2305 - val_loss: 0.2138\n",
      "Epoch 183/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2129 - val_loss: 0.3553\n",
      "Epoch 184/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2041 - val_loss: 0.2120\n",
      "Epoch 185/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2725 - val_loss: 0.3930\n",
      "Epoch 186/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1988 - val_loss: 0.2878\n",
      "Epoch 187/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2268 - val_loss: 0.1887\n",
      "Epoch 188/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3078 - val_loss: 1.3007\n",
      "Epoch 189/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3295 - val_loss: 0.2849\n",
      "Epoch 190/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2730 - val_loss: 0.1869\n",
      "Epoch 191/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1846 - val_loss: 0.1723\n",
      "Epoch 192/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2003 - val_loss: 0.4095\n",
      "Epoch 193/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2853 - val_loss: 0.2731\n",
      "Epoch 194/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1803 - val_loss: 0.2579\n",
      "Epoch 195/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2742 - val_loss: 0.2331\n",
      "Epoch 196/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2045 - val_loss: 0.2336\n",
      "Epoch 197/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 0.2635\n",
      "Epoch 198/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2176 - val_loss: 0.1884\n",
      "Epoch 199/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1952 - val_loss: 0.3553\n",
      "Epoch 200/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2804 - val_loss: 0.1794\n",
      "Epoch 201/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1882 - val_loss: 0.2768\n",
      "Epoch 202/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2429 - val_loss: 0.2592\n",
      "Epoch 203/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2583 - val_loss: 0.1878\n",
      "Epoch 204/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2633 - val_loss: 0.2443\n",
      "Epoch 205/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2020 - val_loss: 0.2236\n",
      "Epoch 206/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1956 - val_loss: 0.1988\n",
      "Epoch 207/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2055 - val_loss: 0.1904\n",
      "Epoch 208/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2644 - val_loss: 0.5057\n",
      "Epoch 209/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2768 - val_loss: 0.2114\n",
      "Epoch 210/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 0.2060\n",
      "Epoch 211/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 0.3202\n",
      "Epoch 212/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1919 - val_loss: 0.1661\n",
      "Epoch 213/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1647 - val_loss: 0.1949\n",
      "Epoch 214/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2008 - val_loss: 0.2352\n",
      "Epoch 215/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2108 - val_loss: 0.2162\n",
      "Epoch 216/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1861 - val_loss: 0.2626\n",
      "Epoch 217/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2026 - val_loss: 0.1592\n",
      "Epoch 218/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1791 - val_loss: 0.1872\n",
      "Epoch 219/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2076 - val_loss: 0.1544\n",
      "Epoch 220/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2350 - val_loss: 0.2503\n",
      "Epoch 221/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1773 - val_loss: 0.2266\n",
      "Epoch 222/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2289 - val_loss: 0.1646\n",
      "Epoch 223/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1595 - val_loss: 0.1934\n",
      "Epoch 224/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2209 - val_loss: 0.2028\n",
      "Epoch 225/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1786 - val_loss: 0.2028\n",
      "Epoch 226/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2295 - val_loss: 0.2961\n",
      "Epoch 227/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2074 - val_loss: 0.1591\n",
      "Epoch 228/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2061 - val_loss: 0.1869\n",
      "Epoch 229/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2044 - val_loss: 0.2408\n",
      "Epoch 230/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1936 - val_loss: 0.1907\n",
      "Epoch 231/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2501 - val_loss: 0.3209\n",
      "Epoch 232/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3134 - val_loss: 0.2448\n",
      "Epoch 233/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1598 - val_loss: 0.1404\n",
      "Epoch 234/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2607 - val_loss: 0.4235\n",
      "Epoch 235/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2009 - val_loss: 0.4106\n",
      "Epoch 236/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1918 - val_loss: 0.2280\n",
      "Epoch 237/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2023 - val_loss: 0.2051\n",
      "Epoch 238/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1854 - val_loss: 0.1707\n",
      "Epoch 239/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1759 - val_loss: 0.1939\n",
      "Epoch 240/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.1361\n",
      "Epoch 241/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1614 - val_loss: 0.1690\n",
      "Epoch 242/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1875 - val_loss: 0.1712\n",
      "Epoch 243/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2402 - val_loss: 0.1625\n",
      "Epoch 244/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1559 - val_loss: 0.1700\n",
      "Epoch 245/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2263 - val_loss: 0.4615\n",
      "Epoch 246/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 0.2612\n",
      "Epoch 247/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1493 - val_loss: 0.2134\n",
      "Epoch 248/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2080 - val_loss: 0.1689\n",
      "Epoch 249/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1496 - val_loss: 0.1731\n",
      "Epoch 250/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.1363\n",
      "Epoch 251/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1539 - val_loss: 0.1608\n",
      "Epoch 252/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2161 - val_loss: 0.3878\n",
      "Epoch 253/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2128 - val_loss: 0.2661\n",
      "Epoch 254/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2012 - val_loss: 0.2524\n",
      "Epoch 255/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1991 - val_loss: 0.1669\n",
      "Epoch 256/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 0.1514\n",
      "Epoch 257/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1762 - val_loss: 0.2921\n",
      "Epoch 258/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1717 - val_loss: 0.1473\n",
      "Epoch 259/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1785 - val_loss: 0.1700\n",
      "Epoch 260/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1852 - val_loss: 0.1484\n",
      "Epoch 261/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1420 - val_loss: 0.1480\n",
      "Epoch 262/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1841 - val_loss: 0.1262\n",
      "Epoch 263/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1523 - val_loss: 0.1472\n",
      "Epoch 264/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1650 - val_loss: 0.1912\n",
      "Epoch 265/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1673 - val_loss: 0.1647\n",
      "Epoch 266/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2047 - val_loss: 0.1576\n",
      "Epoch 267/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2516 - val_loss: 0.2204\n",
      "Epoch 268/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1721 - val_loss: 0.1696\n",
      "Epoch 269/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1906 - val_loss: 0.1557\n",
      "Epoch 270/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1975 - val_loss: 0.2371\n",
      "Epoch 271/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1686 - val_loss: 0.1443\n",
      "Epoch 272/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1822 - val_loss: 0.1747\n",
      "Epoch 273/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1426 - val_loss: 0.1236\n",
      "Epoch 274/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 0.1738\n",
      "Epoch 275/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1859 - val_loss: 0.2012\n",
      "Epoch 276/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1348 - val_loss: 0.3130\n",
      "Epoch 277/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1855 - val_loss: 0.2089\n",
      "Epoch 278/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 0.2425\n",
      "Epoch 279/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1484 - val_loss: 0.1799\n",
      "Epoch 280/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2386 - val_loss: 0.1643\n",
      "Epoch 281/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1369 - val_loss: 0.1358\n",
      "Epoch 282/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1363 - val_loss: 0.1161\n",
      "Epoch 283/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1502\n",
      "Epoch 284/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1636 - val_loss: 0.4459\n",
      "Epoch 285/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1684 - val_loss: 0.5005\n",
      "Epoch 286/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1673 - val_loss: 0.1517\n",
      "Epoch 287/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1438 - val_loss: 0.1366\n",
      "Epoch 288/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1842 - val_loss: 0.1797\n",
      "Epoch 289/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1687 - val_loss: 0.1257\n",
      "Epoch 290/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1489 - val_loss: 0.1123\n",
      "Epoch 291/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.1446\n",
      "Epoch 292/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1514 - val_loss: 0.1322\n",
      "Epoch 293/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.1507\n",
      "Epoch 294/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1473 - val_loss: 0.2182\n",
      "Epoch 295/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1349 - val_loss: 0.1290\n",
      "Epoch 296/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1654 - val_loss: 0.1599\n",
      "Epoch 297/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1754 - val_loss: 0.1857\n",
      "Epoch 298/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1483 - val_loss: 0.1248\n",
      "Epoch 299/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1654 - val_loss: 0.1388\n",
      "Epoch 300/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1809 - val_loss: 0.1314\n",
      "Epoch 301/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1591 - val_loss: 0.1565\n",
      "Epoch 302/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1512 - val_loss: 0.3290\n",
      "Epoch 303/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1476 - val_loss: 0.1324\n",
      "Epoch 304/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2269 - val_loss: 0.1657\n",
      "Epoch 305/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 0.1425\n",
      "Epoch 306/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1307 - val_loss: 0.1475\n",
      "Epoch 307/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1600 - val_loss: 0.1657\n",
      "Epoch 308/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 0.1499\n",
      "Epoch 309/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1395 - val_loss: 0.1610\n",
      "Epoch 310/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.1454\n",
      "Epoch 311/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1867 - val_loss: 0.3404\n",
      "Epoch 312/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 0.1584\n",
      "Epoch 313/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1472 - val_loss: 0.1358\n",
      "Epoch 314/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1509 - val_loss: 0.1801\n",
      "Epoch 315/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1290 - val_loss: 0.1352\n",
      "Epoch 316/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1763 - val_loss: 0.1269\n",
      "Epoch 317/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1458 - val_loss: 0.1664\n",
      "Epoch 318/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1968 - val_loss: 0.2010\n",
      "Epoch 319/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1424 - val_loss: 0.1442\n",
      "Epoch 320/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1335 - val_loss: 0.1868\n",
      "Epoch 321/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.1551\n",
      "Epoch 322/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 0.1570\n",
      "Epoch 323/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1436 - val_loss: 0.1640\n",
      "Epoch 324/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1464 - val_loss: 0.3290\n",
      "Epoch 325/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.2409\n",
      "Epoch 326/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1381 - val_loss: 0.1175\n",
      "Epoch 327/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.1094\n",
      "Epoch 328/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1648 - val_loss: 0.3958\n",
      "Epoch 329/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 0.1369\n",
      "Epoch 330/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1369 - val_loss: 0.2244\n",
      "Epoch 331/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2007 - val_loss: 0.1813\n",
      "Epoch 332/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1610 - val_loss: 0.1746\n",
      "Epoch 333/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1068 - val_loss: 0.1250\n",
      "Epoch 334/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1109 - val_loss: 0.1128\n",
      "Epoch 335/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1449 - val_loss: 0.1167\n",
      "Epoch 336/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1217 - val_loss: 0.1380\n",
      "Epoch 337/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1212 - val_loss: 0.1647\n",
      "Epoch 338/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.0922\n",
      "Epoch 339/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1117 - val_loss: 0.1672\n",
      "Epoch 340/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1355 - val_loss: 0.1411\n",
      "Epoch 341/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1715 - val_loss: 0.3861\n",
      "Epoch 342/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1789 - val_loss: 0.2123\n",
      "Epoch 343/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2036 - val_loss: 0.1364\n",
      "Epoch 344/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1377 - val_loss: 0.1859\n",
      "Epoch 345/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1693 - val_loss: 0.1039\n",
      "Epoch 346/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.0992\n",
      "Epoch 347/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 0.1149\n",
      "Epoch 348/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1123 - val_loss: 0.1268\n",
      "Epoch 349/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1303\n",
      "Epoch 350/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 0.1880\n",
      "Epoch 351/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1124 - val_loss: 0.1334\n",
      "Epoch 352/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1583 - val_loss: 0.1486\n",
      "Epoch 353/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.1940\n",
      "Epoch 354/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1500 - val_loss: 0.1190\n",
      "Epoch 355/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1332\n",
      "Epoch 356/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1414 - val_loss: 0.2059\n",
      "Epoch 357/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.1025\n",
      "Epoch 358/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0886 - val_loss: 0.1147\n",
      "Epoch 359/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1061 - val_loss: 0.1406\n",
      "Epoch 360/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.1407\n",
      "Epoch 361/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1158 - val_loss: 0.1293\n",
      "Epoch 362/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 0.1208\n",
      "Epoch 363/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1229 - val_loss: 0.1530\n",
      "Epoch 364/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 0.1429\n",
      "Epoch 365/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2038 - val_loss: 0.1487\n",
      "Epoch 366/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.1090\n",
      "Epoch 367/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1203 - val_loss: 0.1183\n",
      "Epoch 368/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1393 - val_loss: 0.1469\n",
      "Epoch 369/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.1240\n",
      "Epoch 370/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 0.1581\n",
      "Epoch 371/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.1285\n",
      "Epoch 372/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1193 - val_loss: 0.1263\n",
      "Epoch 373/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1587 - val_loss: 0.3822\n",
      "Epoch 374/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1151 - val_loss: 0.1264\n",
      "Epoch 375/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.1165\n",
      "Epoch 376/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1216 - val_loss: 0.2341\n",
      "Epoch 377/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1187 - val_loss: 0.1574\n",
      "Epoch 378/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.1025\n",
      "Epoch 379/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1102 - val_loss: 0.0914\n",
      "Epoch 380/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.1948\n",
      "Epoch 381/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1359 - val_loss: 0.1601\n",
      "Epoch 382/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1747 - val_loss: 0.1615\n",
      "Epoch 383/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1067 - val_loss: 0.0892\n",
      "Epoch 384/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0889 - val_loss: 0.1090\n",
      "Epoch 385/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1426 - val_loss: 0.2312\n",
      "Epoch 386/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1745 - val_loss: 0.2133\n",
      "Epoch 387/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.1043\n",
      "Epoch 388/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1226 - val_loss: 0.1858\n",
      "Epoch 389/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1020 - val_loss: 0.1121\n",
      "Epoch 390/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.2376\n",
      "Epoch 391/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1577 - val_loss: 0.2497\n",
      "Epoch 392/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1148 - val_loss: 0.1283\n",
      "Epoch 393/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1097 - val_loss: 0.1017\n",
      "Epoch 394/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 0.1070\n",
      "Epoch 395/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.1083\n",
      "Epoch 396/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1365 - val_loss: 0.1706\n",
      "Epoch 397/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1181 - val_loss: 0.1730\n",
      "Epoch 398/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1235 - val_loss: 0.1235\n",
      "Epoch 399/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1566 - val_loss: 0.1005\n",
      "Epoch 400/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0913 - val_loss: 0.1273\n",
      "Epoch 401/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1178 - val_loss: 0.1190\n",
      "Epoch 402/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1010 - val_loss: 0.1754\n",
      "Epoch 403/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1926 - val_loss: 0.3927\n",
      "Epoch 404/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2126 - val_loss: 0.2181\n",
      "Epoch 405/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1725 - val_loss: 0.1412\n",
      "Epoch 406/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 0.1231\n",
      "Epoch 407/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.0996\n",
      "Epoch 408/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0957 - val_loss: 0.1248\n",
      "Epoch 409/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0959 - val_loss: 0.1633\n",
      "Epoch 410/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.1073\n",
      "Epoch 411/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 0.1101\n",
      "Epoch 412/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1139 - val_loss: 0.2080\n",
      "Epoch 413/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1130 - val_loss: 0.2075\n",
      "Epoch 414/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 0.1107\n",
      "Epoch 415/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.1016\n",
      "Epoch 416/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 0.2508\n",
      "Epoch 417/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1608 - val_loss: 0.1432\n",
      "Epoch 418/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1378 - val_loss: 0.1565\n",
      "Epoch 419/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1622 - val_loss: 0.1530\n",
      "Epoch 420/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1154\n",
      "Epoch 421/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1023 - val_loss: 0.0955\n",
      "Epoch 422/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 0.1432\n",
      "Epoch 423/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 0.1792\n",
      "Epoch 424/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1197 - val_loss: 0.1483\n",
      "Epoch 425/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1140 - val_loss: 0.0969\n",
      "Epoch 426/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 0.1206\n",
      "Epoch 427/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1143 - val_loss: 0.0895\n",
      "Epoch 428/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1008 - val_loss: 0.0876\n",
      "Epoch 429/500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1457 - val_loss: 0.1337\n",
      "Epoch 430/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1331 - val_loss: 0.1338\n",
      "Epoch 431/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.0897\n",
      "Epoch 432/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0836 - val_loss: 0.0932\n",
      "Epoch 433/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1125 - val_loss: 0.1165\n",
      "Epoch 434/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1038 - val_loss: 0.2177\n",
      "Epoch 435/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.1077\n",
      "Epoch 436/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0942 - val_loss: 0.0970\n",
      "Epoch 437/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.1649\n",
      "Epoch 438/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0992 - val_loss: 0.1214\n",
      "Epoch 439/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1286 - val_loss: 0.1258\n",
      "Epoch 440/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 0.1289\n",
      "Epoch 441/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0971 - val_loss: 0.1486\n",
      "Epoch 442/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.0820\n",
      "Epoch 443/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0891 - val_loss: 0.0946\n",
      "Epoch 444/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1154 - val_loss: 0.2019\n",
      "Epoch 445/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.0770\n",
      "Epoch 446/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1138 - val_loss: 0.1211\n",
      "Epoch 447/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1168 - val_loss: 0.0866\n",
      "Epoch 448/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.1252\n",
      "Epoch 449/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1122 - val_loss: 0.1358\n",
      "Epoch 450/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0897 - val_loss: 0.1007\n",
      "Epoch 451/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.0926\n",
      "Epoch 452/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1259 - val_loss: 0.1217\n",
      "Epoch 453/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.1963\n",
      "Epoch 454/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.1196\n",
      "Epoch 455/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0977 - val_loss: 0.1099\n",
      "Epoch 456/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0910 - val_loss: 0.0828\n",
      "Epoch 457/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.1346\n",
      "Epoch 458/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.1535\n",
      "Epoch 459/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 0.1569\n",
      "Epoch 460/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.0869\n",
      "Epoch 461/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1031 - val_loss: 0.1094\n",
      "Epoch 462/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1055 - val_loss: 0.1046\n",
      "Epoch 463/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0843 - val_loss: 0.0956\n",
      "Epoch 464/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 0.2118\n",
      "Epoch 465/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1289 - val_loss: 0.0840\n",
      "Epoch 466/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0759 - val_loss: 0.1081\n",
      "Epoch 467/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.1117\n",
      "Epoch 468/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 0.1697\n",
      "Epoch 469/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1167 - val_loss: 0.1431\n",
      "Epoch 470/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1218 - val_loss: 0.1195\n",
      "Epoch 471/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1447 - val_loss: 0.1354\n",
      "Epoch 472/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0860 - val_loss: 0.1004\n",
      "Epoch 473/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 0.1167\n",
      "Epoch 474/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.0845\n",
      "Epoch 475/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.0834\n",
      "Epoch 476/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0933 - val_loss: 0.1567\n",
      "Epoch 477/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0965 - val_loss: 0.0889\n",
      "Epoch 478/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0936 - val_loss: 0.1096\n",
      "Epoch 479/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0929 - val_loss: 0.1056\n",
      "Epoch 480/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1249 - val_loss: 0.1240\n",
      "Epoch 481/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0860 - val_loss: 0.0910\n",
      "Epoch 482/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0761 - val_loss: 0.0768\n",
      "Epoch 483/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0900 - val_loss: 0.0923\n",
      "Epoch 484/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1444 - val_loss: 0.1063\n",
      "Epoch 485/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1318 - val_loss: 0.0985\n",
      "Epoch 486/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.1398\n",
      "Epoch 487/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1136 - val_loss: 0.1169\n",
      "Epoch 488/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.0939\n",
      "Epoch 489/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1785\n",
      "Epoch 490/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.1001\n",
      "Epoch 491/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0902 - val_loss: 0.1818\n",
      "Epoch 492/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.1482\n",
      "Epoch 493/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 0.1884\n",
      "Epoch 494/500\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1108 - val_loss: 0.0983\n",
      "Epoch 495/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 0.1076\n",
      "Epoch 496/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1274 - val_loss: 0.1632\n",
      "Epoch 497/500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1158 - val_loss: 0.0917\n",
      "Epoch 498/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.0766\n",
      "Epoch 499/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0969 - val_loss: 0.1867\n",
      "Epoch 500/500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.0815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2267aa00f90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_valid,y_valid), \n",
    "          epochs=500, \n",
    "          batch_size=25,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSNIR_num = data[:3000-1,0]\n",
    "Input_num = data[:3000-1,1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "OSNIR_est = (model.predict(Input_num)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.0, -3.0, 100.0, 12.5, 0.0, 9.0] => 11.174129 (expected 10.655224)\n",
      "[50.0, -7.0, 50.0, 25.0, 0.0, 3.0] => 15.434303 (expected 14.920383)\n",
      "[30.0, -2.0, 50.0, 25.0, 0.0, 3.0] => 18.075792 (expected 18.138504)\n",
      "[36.0, 1.0, 100.0, 25.0, 0.0, 9.0] => 9.255724 (expected 9.459131)\n",
      "[19.0, -8.0, 10.0, 25.0, 0.0, 9.0] => 25.379766 (expected 25.856131)\n",
      "[24.0, -4.0, 100.0, 12.5, 0.0, 9.0] => 12.648388 (expected 12.804099)\n",
      "[25.0, -7.0, 100.0, 25.0, 0.0, 9.0] => 7.791731 (expected 7.938533)\n",
      "[29.0, 3.0, 100.0, 25.0, 25.0, 9.0] => 9.921718 (expected 9.551508)\n",
      "[27.0, 3.0, 10.0, 50.0, 0.0, 9.0] => 17.149124 (expected 17.620399)\n",
      "[13.0, 3.0, 100.0, 25.0, 12.5, 9.0] => 13.806187 (expected 13.479461)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('%s => %f (expected %f)' % (Input_num[i].tolist(), OSNIR_est[i], OSNIR_num[i] ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate difference between actual OSNIR and predicted OSNIR (in dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mism = np.zeros(3000-1)\n",
    "for i in range(3000-1):\n",
    "    Mism[i] = OSNIR_num[i] - OSNIR_est[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Actual OSNIR (dB)\" : OSNIR_num.tolist(), \"Predicted OSNIR (dB)\" : OSNIR_est.tolist(), \"Difference (dB)\" : Mism.tolist()})\n",
    "pred_df.to_csv(\"OSNIR_Prediction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00033344,\n",
       "        0.00066689, 0.00166722, 0.009003  , 0.02667556, 0.05568523,\n",
       "        0.1057019 , 0.16272091, 0.21407136, 0.17972658, 0.12737579,\n",
       "        0.07035679, 0.02834278, 0.01133711, 0.003001  , 0.00100033,\n",
       "        0.00133378, 0.        , 0.        , 0.00033344, 0.00033344,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00033344, 0.        , 0.        , 0.        ]),\n",
       " array([-3.        , -2.86666667, -2.73333333, -2.6       , -2.46666667,\n",
       "        -2.33333333, -2.2       , -2.06666667, -1.93333333, -1.8       ,\n",
       "        -1.66666667, -1.53333333, -1.4       , -1.26666667, -1.13333333,\n",
       "        -1.        , -0.86666667, -0.73333333, -0.6       , -0.46666667,\n",
       "        -0.33333333, -0.2       , -0.06666667,  0.06666667,  0.2       ,\n",
       "         0.33333333,  0.46666667,  0.6       ,  0.73333333,  0.86666667,\n",
       "         1.        ,  1.13333333,  1.26666667,  1.4       ,  1.53333333,\n",
       "         1.66666667,  1.8       ,  1.93333333,  2.06666667,  2.2       ,\n",
       "         2.33333333,  2.46666667,  2.6       ,  2.73333333,  2.86666667,\n",
       "         3.        ]),\n",
       " <BarContainer object of 45 artists>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Deep Neural Network OSNIR Mismatch')\n",
    "plt.xlabel('Mismatch (dB)')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.hist(Mism, range=(-3,3), rwidth=0.9, bins=45, weights=np.ones(len(Mism))/len(Mism))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd7dbc09f62c6934dc245b76251cec5fd949cd1cb8bcb775af9208cac5a10da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
